{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedmode_name=\"/model_seed42_score0.991.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --force-reinstall /kaggle/input/unidecode/Unidecode-1.3.8-py3-none-any.whl\n",
    "# %cp /kaggle/input/utility/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:change path\n",
    "from cfg import *\n",
    "from util import *\n",
    "from model1 import NLPModel\n",
    "import gc\n",
    "\n",
    "# Config.data_path='/kaggle/input/pii-detection-removal-from-educational-data/'\n",
    "# Config.modelsavepath='/kaggle/input/deberbase0213'\n",
    "tokenizer_path=Config.modelsavepath\n",
    "modelpath=Config.modelsavepath+savedmode_name\n",
    "modelpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.modelsavepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(Config.data_path+\"/train.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [str(x[\"document\"]) for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "idlabel= json.load(open(Config.modelsavepath+\"//idlabel.json\"))\n",
    "id2label=idlabel['id2label']\n",
    "label2id=idlabel['label2id']\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_preprocesss(example,tokenizer,label2id):\n",
    "    # rebuild text from tokens\n",
    "\n",
    "    example['token_map']=[]\n",
    "    example['berttokenids']=[]\n",
    "    example['berttokenmask']=[]\n",
    "    example['berttokentoken_type_ids']=[]\n",
    "    example['offset_mapping']=[]\n",
    "\n",
    "    text = []\n",
    "    token_map = []\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        \n",
    "        text.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "            \n",
    "        idx += 1\n",
    "        \n",
    "        \n",
    "        # actual tokenization\n",
    "    tokenizeds = tokenizer(\"\".join(text),\n",
    "                        stride=Config.stride,\n",
    "                        max_length=Config.max_length,\n",
    "                        truncation=True ,\n",
    "                        return_overflowing_tokens=True,\n",
    "                        return_offsets_mapping=True,\n",
    "                        )\n",
    "\n",
    "\n",
    "    example['berttokenids']=(tokenizeds['input_ids'])\n",
    "    example['berttokenmask']=(tokenizeds['attention_mask'])\n",
    "    example['berttokentoken_type_ids']=(tokenizeds['token_type_ids'])\n",
    "    example['offset_mapping']=(tokenizeds['offset_mapping'])\n",
    "    example['token_map']=(token_map)\n",
    "        \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesssed_ds=ds.map(inference_preprocesss, fn_kwargs={'tokenizer':tokenizer,'label2id':label2id},num_proc=10,desc=\"prepocessing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuid dataset \n",
    "tmp_pd=expanddataset(preprocesssed_ds,if_train=False)\n",
    "print(tmp_pd['berttokenids'].str.len().agg(['mean','max','std','min']))\n",
    "print(len(tmp_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds=Dataset.from_pandas(tmp_pd)\n",
    "full_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=NLPModel(id2label,label2id,Config.modelsavepath,training=False).to(device)\n",
    "model.load_state_dict(torch.load(modelpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = Collate(tokenizer=tokenizer,if_train=False)\n",
    "val_dataloader=DataLoader(full_ds,batch_size=1,pin_memory=True,collate_fn=data_collator)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_before_pading_logit(batch_logits,batch_org_len):\n",
    "    \"\"\"\n",
    "        按顺序返回pading前的logit列表\n",
    "    \"\"\"\n",
    "    logit_list=[]\n",
    "    batch_len=max(batch_org_len)\n",
    "    for i,l in enumerate(batch_org_len):\n",
    "        loggit=batch_logits[i*batch_len:i*batch_len+l]\n",
    "        logit_list.append(loggit)\n",
    "    return logit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "logits=[]\n",
    "for step,dataset in enumerate(tqdm(val_dataloader)):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ids = dataset[\"ids\"].to(device,non_blocking=True)\n",
    "        mask = dataset[\"mask\"].to(device,non_blocking=True)\n",
    "        tokentype = dataset[\"type_ids\"].to(device,non_blocking=True)\n",
    "        logit,loss = model(ids,mask,tokentype)\n",
    "        logits+=(ret_before_pading_logit(logit,dataset['token_org_length']))\n",
    "        del ids,mask,tokentype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_map = {}\n",
    "document, token, label, token_str = [], [], [], []\n",
    "\n",
    "for p,token_map, offsets,tokens, doc in zip(logits,full_ds[\"token_map\"], full_ds['offset_mapping'],full_ds[\"tokens\"], full_ds[\"document\"]):\n",
    "\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        \n",
    "        if start_idx + end_idx == 0: continue\n",
    "        if token_map[start_idx] == -1:\n",
    "            start_idx += 1\n",
    "\n",
    "         # ignore \"\\n\\n\"\n",
    "        while start_idx < len(token_map) and  tokens[token_map[start_idx]].isspace():\n",
    "            start_idx += 1\n",
    "\n",
    "        if start_idx >= len(token_map): break\n",
    "\n",
    "        token_id = token_map[start_idx]\n",
    "\n",
    "        # ignore \"O\" predictions and whitespace preds\n",
    "        if token_id != -1:\n",
    "            triplet_key = (doc,token_id,tokens[token_id])\n",
    "\n",
    "            if triplet_key not in triplet_map:\n",
    "                triplet_map[triplet_key]=[token_pred]\n",
    "            else:\n",
    "                triplet_map[triplet_key].append(token_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in triplet_map:\n",
    "    averged_logit=np.mean(triplet_map[key],axis=0)\n",
    "    pred_softmax= np.exp(averged_logit) / np.sum(np.exp(averged_logit))\n",
    "    if pred_softmax[12]> Config.threshold:\n",
    "        true_predict='O'\n",
    "    else:\n",
    "        true_predict=id2label[str(pred_softmax[:12].argmax())]\n",
    "    triplet_map[key]=true_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(triplet_map.items(), columns=['triplet_map', 'label'])\n",
    "df=df.loc[df['label']!='O'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['document'],df['token'], df['token_str'] =  zip(*df.triplet_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"row_id\"] = list(range(len(df)))\n",
    "(df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
