{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedmode_name=\"/model_seed2023_score0.984.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --force-reinstall /kaggle/input/unidecode/Unidecode-1.3.8-py3-none-any.whl\n",
    "%cp /kaggle/input/utility/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:change path\n",
    "from cfg import *\n",
    "from util import *\n",
    "from model1 import NLPModel\n",
    "import gc\n",
    "\n",
    "# Config.data_path='/kaggle/input/pii-detection-removal-from-educational-data/'\n",
    "# Config.modelsavepath='/kaggle/input/deberbase0213'\n",
    "tokenizer_path=Config.modelsavepath\n",
    "modelpath=Config.modelsavepath+savedmode_name\n",
    "modelpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.modelsavepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(Config.data_path+\"/train.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [str(x[\"document\"]) for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "idlabel= json.load(open(Config.modelsavepath+\"//idlabel.json\"))\n",
    "id2label=idlabel['id2label']\n",
    "label2id=idlabel['label2id']\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_preprocesss(example,tokenizer,label2id):\n",
    "    # rebuild text from tokens\n",
    "\n",
    "    example['token_map']=[]\n",
    "    example['berttokenids']=[]\n",
    "    example['berttokenmask']=[]\n",
    "    example['berttokentoken_type_ids']=[]\n",
    "    example['offset_mapping']=[]\n",
    "\n",
    "    tokens_split_list=[]\n",
    "    trailing_whitespace_split_list=[]\n",
    "\n",
    "    right_idx=0\n",
    "    for i in range(0,len(example['tokens'])):\n",
    "        if example['tokens'][i] == '\\n\\n':\n",
    "            tokens_split_list.append(example['tokens'][right_idx:i+1])\n",
    "            trailing_whitespace_split_list.append(example['trailing_whitespace'][right_idx:i+1])\n",
    "            right_idx=i+1\n",
    "\n",
    "    if  len(tokens_split_list)==0:\n",
    "        tokens_split_list.append( example['tokens'])\n",
    "        trailing_whitespace_split_list.append( example['trailing_whitespace'])\n",
    "\n",
    "    idx=0\n",
    "    for tokens_list,trailing_whitespace_list in zip(\n",
    "        tokens_split_list,trailing_whitespace_split_list):\n",
    "\n",
    "        text = []\n",
    "        token_map = []\n",
    "\n",
    "        for t,  ws in zip(\n",
    "            tokens_list, trailing_whitespace_list\n",
    "        ):\n",
    "            text.append(t)\n",
    "            token_map.extend([idx]*len(t))\n",
    "\n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "                token_map.append(-1)\n",
    "            idx += 1\n",
    "        # actual tokenization\n",
    "        tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True)\n",
    "\n",
    "        example['berttokenids'].append(tokenized['input_ids'])\n",
    "        example['berttokenmask'].append(tokenized['attention_mask'])\n",
    "        example['berttokentoken_type_ids'].append(tokenized['token_type_ids'])\n",
    "        example['offset_mapping'].append(tokenized['offset_mapping'])\n",
    "        example['token_map'].append(token_map)\n",
    "        \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesssed_ds=ds.map(inference_preprocesss, fn_kwargs={'tokenizer':tokenizer,'label2id':label2id},num_proc=10,desc=\"prepocessing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuid dataset \n",
    "tmp_pd=expanddataset(preprocesssed_ds,if_train=False)\n",
    "print(tmp_pd['berttokenids'].str.len().agg(['mean','max','std','min']))\n",
    "tmp_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds=Dataset.from_pandas(tmp_pd)\n",
    "full_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=NLPModel(id2label,label2id,Config.modelsavepath,training=False).to(device)\n",
    "model.load_state_dict(torch.load(modelpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = Collate(tokenizer=tokenizer,if_train=False)\n",
    "val_dataloader=DataLoader(full_ds,batch_size=5,pin_memory=True,collate_fn=data_collator)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "allprecitions=[]\n",
    "for step,dataset in enumerate(tqdm(val_dataloader)):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ids = dataset[\"ids\"].to(device,non_blocking=True)\n",
    "        mask = dataset[\"mask\"].to(device,non_blocking=True)\n",
    "        tokentype = dataset[\"type_ids\"].to(device,non_blocking=True)\n",
    "        logit,loss = model(ids,mask,tokentype)\n",
    "        allprecitions+=(logit2truepredic(logit,dataset['token_org_length']))\n",
    "        del ids,mask,tokentype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allprecitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = set()\n",
    "document, token, label, token_str = [], [], [], []\n",
    "\n",
    "for p,token_map, offsets,tokens, doc in zip(allprecitions,full_ds[\"token_map\"], full_ds['offset_mapping'],full_ds[\"tokens\"], full_ds[\"document\"]):\n",
    "\n",
    "    tmp_id=0\n",
    "    assert(len(p)==len(offsets))\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        \n",
    "        label_pred = id2label[str(token_pred)]\n",
    "        if start_idx + end_idx == 0: continue\n",
    "        if token_map[start_idx] == -1:\n",
    "            start_idx += 1\n",
    "\n",
    "         # ignore \"\\n\\n\"\n",
    "        while start_idx < len(token_map) and  tokens[token_map[start_idx]].isspace():\n",
    "            start_idx += 1\n",
    "\n",
    "        if start_idx >= len(token_map): break\n",
    "\n",
    "        token_id = token_map[start_idx]\n",
    "\n",
    "        # ignore \"O\" predictions and whitespace preds\n",
    "        if label_pred != \"O\" and token_id != -1:\n",
    "            triplet = (label_pred, token_id, tokens[token_id])\n",
    "\n",
    "            if triplet not in triplets:\n",
    "                document.append(doc)\n",
    "                token.append(token_id)\n",
    "                label.append(label_pred)\n",
    "                token_str.append(tokens[token_id])\n",
    "                triplets.add(triplet)\n",
    "        tmp_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"document\": document,\n",
    "    \"token\": token,\n",
    "    \"label\": label,\n",
    "    \"token_str\": token_str\n",
    "})\n",
    "df[\"row_id\"] = list(range(len(df)))\n",
    "display(df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
