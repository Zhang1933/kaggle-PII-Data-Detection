{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedmodel_name=\"/seed42_e3_vs545_score0.926.pth\"\n",
    "savemode_dir = \"/kaggle/input/seed42_score0.991/pytorch/v41/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np#进行矩阵运算的库\n",
    "import random#提供了一些用于生成随机数的函数\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "import copy\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "\n",
    "\n",
    "class Config:\n",
    "\n",
    "    num_proc=10\n",
    "    seed=42 #随机种子\n",
    "\n",
    "    split_by_paragraph=True #False to split by sentence windows\n",
    "    max_length=1024 # for sentence sliding windows \n",
    "    stride=256 # overlap count\n",
    "\n",
    "\n",
    "    batch_size=3 # TODO:change batch size\n",
    "    logging_steps=100\n",
    "    epochs=3\n",
    "    lr=2e-5\n",
    "    weight_decay=0.01\n",
    "    accumulation_steps=1 # batch size不同，不能直接除。有空再修\n",
    "    evaltimes=3\n",
    "    num_warmup_steps=0\n",
    "\n",
    "    resume_train_epoch=0\n",
    "\n",
    " \n",
    "\n",
    "#设置随机种子,保证模型可以复现\n",
    "def seed_everything():\n",
    "    seed=Config.seed\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# batch 对齐\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer,if_train=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.if_train=if_train\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"ids\"] = [sample[\"berttokenids\"] for sample in batch]\n",
    "        output[\"type_ids\"] = [sample[\"berttokentoken_type_ids\"] for sample in batch]\n",
    "        output[\"mask\"] = [sample[\"berttokenmask\"] for sample in batch]\n",
    "        if self.if_train: \n",
    "            output[\"targets\"] = [sample[\"bertlabels\"] for sample in batch]\n",
    "        output[\"token_org_length\"]=[len(ids) for ids in output[\"ids\"]]\n",
    "\n",
    "         # calculate max token length of this batch\n",
    "        batch_max = max(output[\"token_org_length\"])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n",
    "            output[\"type_ids\"] = [s + (batch_max - len(s)) * [0] for s in output[\"type_ids\"]]\n",
    "            if self.if_train: output[\"targets\"] = [s + (batch_max - len(s)) * [-100] for s in output[\"targets\"]]\n",
    "        else:\n",
    "            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n",
    "            output[\"type_ids\"] = [(batch_max - len(s)) * [0] + s for s in output[\"type_ids\"]]\n",
    "            if self.if_train: output[\"targets\"] = [(batch_max - len(s)) * [-100] + s for s in output[\"targets\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n",
    "        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n",
    "        output[\"type_ids\"] = torch.tensor(output[\"type_ids\"], dtype=torch.long)\n",
    "        if self.if_train: output[\"targets\"] = torch.tensor(output[\"targets\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "\n",
    "def text_precessor(data):\n",
    "    # 预处理\n",
    "    # 文本转小写,unicode 轉換ascii\n",
    "    # 转换失败转'*'\n",
    "    data=copy.deepcopy(data)\n",
    "    for i in tqdm(range(len(data)),desc=\"preprocess texe\"):\n",
    "        data[i]['full_text']=unidecode(data[i]['full_text'],errors='replace',replace_str='*')\n",
    "        data[i]['full_text']=data[i]['full_text'].lower()\n",
    "        for j in range(len(data[i]['tokens'])):\n",
    "            data[i]['tokens'][j]=unidecode(data[i]['tokens'][j],errors='replace',replace_str='*')  \n",
    "            data[i]['tokens'][j]=data[i]['tokens'][j].lower().strip()\n",
    "    return data\n",
    "\n",
    "def split_by_paragraph(text):\n",
    "    res=[i+'\\n\\n' for i in  text.split('\\n\\n')]\n",
    "    return res\n",
    "\n",
    "# 分句子\n",
    "def split_token(text):\n",
    "    \"\"\"\n",
    "        分句子，返回列表\n",
    "    \"\"\"\n",
    "    data_out=[]\n",
    "    if Config.split_by_paragraph:\n",
    "        doc=split_by_paragraph(text)\n",
    "        return doc\n",
    "    else:\n",
    "        doc=sent_tokenize(text)\n",
    "    split_sentence=doc\n",
    "    length=len(split_sentence)\n",
    "    idx=0\n",
    "    while idx+Config.max_sen_count<length:\n",
    "        split_sentence[idx+Config.max_sen_count-Config.stride]=\"[SEP]\"+split_sentence[idx+Config.max_sen_count-Config.stride]\n",
    "        if idx==0:\n",
    "            data_out.append(\"\".join(split_sentence[idx:idx+Config.max_sen_count]))\n",
    "        else:\n",
    "            data_out.append(\"\".join(split_sentence[idx-Config.stride:idx+Config.max_sen_count]))\n",
    "        idx+=Config.max_sen_count\n",
    "    # deal with reminder\n",
    "    if idx<Config.max_sen_count:# few sentences,concatenate directly\n",
    "        data_out.append(\"\".join(split_sentence[idx:length]))\n",
    "    else:\n",
    "        data_out.append(\"\".join(split_sentence[idx-Config.stride:length]))\n",
    "    return data_out\n",
    "\n",
    "def train_preprocesss(example,tokenizer,label2id):\n",
    "    # rebuild text from tokens\n",
    "\n",
    "    example['bertlabels']=[]\n",
    "    example['berttokenids']=[]\n",
    "    example['berttokenmask']=[]\n",
    "    example['berttokentoken_type_ids']=[]\n",
    "\n",
    "    # rebuild text from tokens\n",
    "    text = []\n",
    "    labels = []\n",
    "\n",
    "    for t, l, ws in zip(\n",
    "        example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]\n",
    "    ):\n",
    "        text.append(t)\n",
    "        labels.extend([l] * len(t))\n",
    "\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "\n",
    "    # actual tokenization\n",
    "    tokenizeds = tokenizer(\"\".join(text),\n",
    "                        stride=Config.stride,\n",
    "                        max_length=Config.max_length,\n",
    "                        truncation=True ,\n",
    "                        return_overflowing_tokens=True,\n",
    "                        return_offsets_mapping=True,\n",
    "                        )\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    \n",
    "    \n",
    "    for  i in range(len( tokenizeds['input_ids'])):\n",
    "        token_labels = []\n",
    "\n",
    "        for start_idx, end_idx in tokenizeds['offset_mapping'][i]:\n",
    "            # CLS token\n",
    "            if start_idx == 0 and end_idx == 0:\n",
    "                token_labels.append(label2id[\"O\"])\n",
    "                continue\n",
    "\n",
    "            # case when token starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            token_labels.append(label2id[labels[start_idx]])\n",
    "\n",
    "        example['bertlabels'].append(token_labels)\n",
    "        example['berttokenids'].append(tokenizeds['input_ids'][i])\n",
    "        example['berttokenmask'].append(tokenizeds['attention_mask'][i])\n",
    "        example['berttokentoken_type_ids'].append(tokenizeds['token_type_ids'][i])\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "def expanddataset(ds,if_train=True):\n",
    "    \"\"\"\n",
    "        将dataset的 bertlabels,berttokenpos2orgtokenpos expand到列,返回新的dataset\n",
    "    \"\"\"\n",
    "    df=ds.to_pandas()\n",
    "    merge_list_key=[]\n",
    "    # s1 = pd.DataFrame(df.pop('bertlabels').values.tolist(), \n",
    "    #       index=df.index).stack().rename('bertlabels').reset_index(level=1, drop=True)\n",
    "    if if_train :\n",
    "        merge_list_key=['berttokenids','berttokenmask','berttokentoken_type_ids','bertlabels']\n",
    "    else:\n",
    "        merge_list_key=['berttokenids','berttokenmask','berttokentoken_type_ids','offset_mapping']\n",
    "    s_l=[]\n",
    "    for i in merge_list_key:\n",
    "        tmp_s= pd.DataFrame(df.pop(i).values.tolist(), \n",
    "                    index=df.index).stack().rename(i).reset_index(level=1, drop=True)\n",
    "        s_l.append(tmp_s)\n",
    "    df = df.join(pd.concat(s_l, axis=1))\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def logit2truepredic(batch_predictions,batch_org_len):\n",
    "    \"\"\"\n",
    "        按顺序返回列表\n",
    "    \"\"\"\n",
    "\n",
    "    preds_final=[]\n",
    "    batch_len=max(batch_org_len)\n",
    "    for i,l in enumerate(batch_org_len):\n",
    "        predictions=batch_predictions[i*batch_len:i*batch_len+l]\n",
    "\n",
    "        softmaxed_pred=np.exp(predictions) / np.sum(np.exp(predictions), axis = 1).reshape(-1,1)\n",
    "        preds = predictions.argmax(-1)\n",
    "        preds_without_O = softmaxed_pred[:,:12].argmax(-1)\n",
    "        O_preds = predictions[:,12]\n",
    "        preds_final.append( list(np.where(O_preds < Config.threshold, preds_without_O , preds)))\n",
    "\n",
    "\n",
    "    return preds_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n",
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [str(x[\"document\"]) for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(savemode_dir)\n",
    "idlabel= json.load(open(savemode_dir+\"/idlabel.json\"))\n",
    "id2label=idlabel['id2label']\n",
    "label2id=idlabel['label2id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_preprocesss(example,tokenizer,label2id):\n",
    "    # rebuild text from tokens\n",
    "\n",
    "    example['token_map']=[]\n",
    "    example['berttokenids']=[]\n",
    "    example['berttokenmask']=[]\n",
    "    example['berttokentoken_type_ids']=[]\n",
    "    example['offset_mapping']=[]\n",
    "\n",
    "    text = []\n",
    "    token_map = []\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        \n",
    "        text.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "            \n",
    "        idx += 1\n",
    "        \n",
    "        \n",
    "        # actual tokenization\n",
    "    tokenizeds = tokenizer(\"\".join(text),\n",
    "                        stride=Config.stride,\n",
    "                        max_length=Config.max_length,\n",
    "                        truncation=True ,\n",
    "                        return_overflowing_tokens=True,\n",
    "                        return_offsets_mapping=True,\n",
    "                        )\n",
    "\n",
    "\n",
    "    example['berttokenids']=(tokenizeds['input_ids'])\n",
    "    example['berttokenmask']=(tokenizeds['attention_mask'])\n",
    "    example['berttokentoken_type_ids']=(tokenizeds['token_type_ids'])\n",
    "    example['offset_mapping']=(tokenizeds['offset_mapping'])\n",
    "    example['token_map']=(token_map)\n",
    "        \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesssed_ds=ds.map(inference_preprocesss, fn_kwargs={'tokenizer':tokenizer,'label2id':label2id},num_proc=10,desc=\"prepocessing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuid dataset \n",
    "tmp_pd=expanddataset(preprocesssed_ds,if_train=False)\n",
    "print(tmp_pd['berttokenids'].str.len().agg(['mean','max','std','min']))\n",
    "print(len(tmp_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds=Dataset.from_pandas(tmp_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForTokenClassification, AutoModel\n",
    "\n",
    "class NLPModel(nn.Module):\n",
    "    def __init__(self,id2label,label2id,modelname,training=True):\n",
    "        super().__init__()\n",
    "    #     self.model=AutoModelForTokenClassification.from_pretrained(modelname, num_labels=len(id2label),\n",
    "    # id2label=id2label,label2id=label2id,ignore_mismatched_sizes=True)#label映射到id\n",
    "        self.model = AutoModel.from_pretrained(modelname, # num_labels=len(id2label),\n",
    "    id2label=id2label,label2id=label2id,ignore_mismatched_sizes=True)\n",
    "        self.lossfunc = nn.CrossEntropyLoss()\n",
    "        self.num_labels=len(id2label)\n",
    "        if training: self.model.save_pretrained(Config.modelsavepath)\n",
    "\n",
    "        hidden_dim = 1024\n",
    "        self.lstm = nn.LSTM(input_size = hidden_dim ,\n",
    "                            hidden_size = hidden_dim  // 2,\n",
    "                            num_layers = 1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            # dropout=0.1\n",
    "                        )\n",
    "        self.l = nn.Linear(hidden_dim, self.num_labels)\n",
    "\n",
    "    def forward(self,input_ids, token_type_ids,attention_mask,labels=None):\n",
    "        output = self.model(input_ids, token_type_ids, attention_mask)\n",
    "        # batchsize*seq length\n",
    "        # logit=output[0]\n",
    "\n",
    "        self.lstm.flatten_parameters()\n",
    "        hidden, (_, _) = self.lstm(output[0])\n",
    "        logit = self.l(hidden)\n",
    "        loss=0\n",
    "        logit=logit.view(-1,logit.shape[-1])\n",
    "        if labels is not None:\n",
    "             labels=labels.view(-1)\n",
    "             loss=self.lossfunc(logit,labels)\n",
    "        \n",
    "        \n",
    "        logit=logit.detach().cpu().numpy()\n",
    "        return logit,loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=NLPModel(id2label,label2id,\"/kaggle/input/deberta-v3-large/pytorch/deberta-large/1\",training=False).to(device)\n",
    "model.load_state_dict(torch.load(savemode_dir+savedmodel_name, map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesssed_ds=ds.map(inference_preprocesss, fn_kwargs={'tokenizer':tokenizer,'label2id':label2id},num_proc=10,desc=\"prepocessing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuid dataset \n",
    "tmp_pd=expanddataset(preprocesssed_ds,if_train=False)\n",
    "print(tmp_pd['berttokenids'].str.len().agg(['mean','max','std','min']))\n",
    "print(len(tmp_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds=Dataset.from_pandas(tmp_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForTokenClassification, AutoModel\n",
    "\n",
    "class NLPModel(nn.Module):\n",
    "    def __init__(self,id2label,label2id,modelname,training=True):\n",
    "        super().__init__()\n",
    "    #     self.model=AutoModelForTokenClassification.from_pretrained(modelname, num_labels=len(id2label),\n",
    "    # id2label=id2label,label2id=label2id,ignore_mismatched_sizes=True)#label映射到id\n",
    "        self.model = AutoModel.from_pretrained(modelname, # num_labels=len(id2label),\n",
    "    id2label=id2label,label2id=label2id,ignore_mismatched_sizes=True)\n",
    "        self.lossfunc = nn.CrossEntropyLoss()\n",
    "        self.num_labels=len(id2label)\n",
    "        if training: self.model.save_pretrained(Config.modelsavepath)\n",
    "\n",
    "        hidden_dim = 1024\n",
    "        self.lstm = nn.LSTM(input_size = hidden_dim ,\n",
    "                            hidden_size = hidden_dim  // 2,\n",
    "                            num_layers = 1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            # dropout=0.1\n",
    "                        )\n",
    "        self.l = nn.Linear(hidden_dim, self.num_labels)\n",
    "\n",
    "    def forward(self,input_ids, token_type_ids,attention_mask,labels=None):\n",
    "        output = self.model(input_ids, token_type_ids, attention_mask)\n",
    "        # batchsize*seq length\n",
    "        # logit=output[0]\n",
    "\n",
    "        self.lstm.flatten_parameters()\n",
    "        hidden, (_, _) = self.lstm(output[0])\n",
    "        logit = self.l(hidden)\n",
    "        loss=0\n",
    "        logit=logit.view(-1,logit.shape[-1])\n",
    "        if labels is not None:\n",
    "             labels=labels.view(-1)\n",
    "             loss=self.lossfunc(logit,labels)\n",
    "        \n",
    "        \n",
    "        logit=logit.detach().cpu().numpy()\n",
    "        return logit,loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=NLPModel(id2label,label2id,\"/kaggle/input/deberta-v3-large/pytorch/deberta-large/1\",training=False).to(device)\n",
    "model.load_state_dict(torch.load(savemode_dir+savedmodel_name, map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = Collate(tokenizer=tokenizer,if_train=False)\n",
    "val_dataloader=DataLoader(full_ds,batch_size=1,pin_memory=True,collate_fn=data_collator)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_before_pading_logit(batch_logits,batch_org_len):\n",
    "    \"\"\"\n",
    "        按顺序返回pading前的logit列表\n",
    "    \"\"\"\n",
    "    logit_list=[]\n",
    "    batch_len=max(batch_org_len)\n",
    "    for i,l in enumerate(batch_org_len):\n",
    "        loggit=batch_logits[i*batch_len:i*batch_len+l]\n",
    "        logit_list.append(loggit)\n",
    "    return logit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "logits=[]\n",
    "for step,dataset in enumerate(tqdm(val_dataloader)):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ids = dataset[\"ids\"].to(device,non_blocking=True)\n",
    "        mask = dataset[\"mask\"].to(device,non_blocking=True)\n",
    "        tokentype = dataset[\"type_ids\"].to(device,non_blocking=True)\n",
    "        logit,loss = model(ids,mask,tokentype)\n",
    "        logits+=(ret_before_pading_logit(logit,dataset['token_org_length']))\n",
    "        del ids,mask,tokentype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_map = {}\n",
    "document, token, label, token_str = [], [], [], []\n",
    "\n",
    "for p,token_map, offsets,tokens, doc in zip(logits,full_ds[\"token_map\"], full_ds['offset_mapping'],full_ds[\"tokens\"], full_ds[\"document\"]):\n",
    "\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        \n",
    "        if start_idx + end_idx == 0: continue\n",
    "        if token_map[start_idx] == -1:\n",
    "            start_idx += 1\n",
    "\n",
    "         # ignore \"\\n\\n\"\n",
    "        while start_idx < len(token_map) and  tokens[token_map[start_idx]].isspace():\n",
    "            start_idx += 1\n",
    "\n",
    "        if start_idx >= len(token_map): break\n",
    "\n",
    "        token_id = token_map[start_idx]\n",
    "\n",
    "        # ignore \"O\" predictions and whitespace preds\n",
    "        if token_id != -1:\n",
    "            triplet_key = (doc,token_id,tokens[token_id])\n",
    "\n",
    "            if triplet_key not in triplet_map:\n",
    "                triplet_map[triplet_key]=[token_pred]\n",
    "            else:\n",
    "                triplet_map[triplet_key].append(token_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.threshold=0.9875\n",
    "for key in triplet_map:\n",
    "    averged_logit=np.mean(triplet_map[key],axis=0)\n",
    "    pred_softmax= np.exp(averged_logit) / np.sum(np.exp(averged_logit))\n",
    "    if pred_softmax[12]> Config.threshold:\n",
    "        true_predict='O'\n",
    "    else:\n",
    "        true_predict=id2label[str(pred_softmax[:12].argmax())]\n",
    "    triplet_map[key]=true_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(triplet_map.items(), columns=['triplet_map', 'label'])\n",
    "df=df.loc[df['label']!='O'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['document'],df['token'], df['token_str'] =  zip(*df.triplet_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"row_id\"] = list(range(len(df)))\n",
    "(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
