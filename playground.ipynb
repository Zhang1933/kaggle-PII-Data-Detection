{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "# output = result.stdout\n",
    "# for line in output.splitlines():\n",
    "#     if '=' in line:\n",
    "#         var, value = line.split('=', 1)\n",
    "#         os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir -p modelsave checkpoint/\n",
    "# %pip install --force-reinstall /home/z1933/workplace/machinelearning/PII-Data-Detection/unidecode/Unidecode-1.3.8-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfg import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original datapoints:  6807\n",
      "external datapoints:  4434\n",
      "moredata datapoints:  2000\n",
      "combined:  9333\n"
     ]
    }
   ],
   "source": [
    "##### main\n",
    "data = json.load(open(Config.data_path+\"/train.json\"))\n",
    "\n",
    "# downsampling of negative examples\n",
    "p=[] # positive samples (contain relevant labels)\n",
    "n=[] # negative samples (presumably contain entities that are possibly wrongly classified as entity)\n",
    "for d in data:\n",
    "    if any(np.array(d[\"labels\"]) != \"O\"): p.append(d)\n",
    "    else: n.append(d)\n",
    "print(\"original datapoints: \", len(data))\n",
    "\n",
    "external = json.load(open(Config.data_path+\"/pii_dataset_fixed.json\"))\n",
    "print(\"external datapoints: \", len(external))\n",
    "\n",
    "moredata = json.load(open(Config.data_path+\"/moredata_dataset_fixed.json\"))\n",
    "print(\"moredata datapoints: \", len(moredata))\n",
    "\n",
    "data = moredata+external+p+n[:len(n)//3]\n",
    "print(\"combined: \", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd=pd.DataFrame(data)\n",
    "len(data_pd[data_pd['tokens'].apply(len)>2024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/z1933/workplace/machinelearning/PII-Data-Detection//modelsave/'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.modelsavepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document                                                           10078\n",
       "full_text              Overcoming Barriers - The Story of the Movie a...\n",
       "tokens                 [Overcoming, Barriers, -, The, Story, of, the,...\n",
       "trailing_whitespace    [True, True, True, True, True, True, True, Tru...\n",
       "labels                 [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "Name: 7919, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd.iloc[data_pd.full_text.str.len().idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z1933/miniconda3/envs/d2l/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
    "tokenized_input =tokenizer(data_pd.iloc[data_pd.full_text.str.len().idxmax()]['full_text'],return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2831])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "len(tokenized_input[\"input_ids\"])\n",
    "tokenized_input[\"input_ids\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_labels:['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'B-EMAIL',\n",
       " 1: 'B-ID_NUM',\n",
       " 2: 'B-NAME_STUDENT',\n",
       " 3: 'B-PHONE_NUM',\n",
       " 4: 'B-STREET_ADDRESS',\n",
       " 5: 'B-URL_PERSONAL',\n",
       " 6: 'B-USERNAME',\n",
       " 7: 'I-ID_NUM',\n",
       " 8: 'I-NAME_STUDENT',\n",
       " 9: 'I-PHONE_NUM',\n",
       " 10: 'I-STREET_ADDRESS',\n",
       " 11: 'I-URL_PERSONAL',\n",
       " 12: 'O'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "#将所有数据的labels连接在一起,然后查重,转成list的格式,然后从小到大排序\n",
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "print(f\"all_labels:{all_labels}\")\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "#这个是{id:label},上面是{label:id}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "# Open a file for writing\n",
    "with open(Config.modelsavepath+\"//idlabel.json\", \"w\") as f:\n",
    "    # Write the map to the file in JSON format\n",
    "    json.dump({'label2id':label2id,'id2label':id2label}, f)\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_df):6807, train_df[0].keys(): ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels']\n",
      "--------------------------------------------------\n",
      "train_df length: 6807\n",
      "labels: {'I-NAME_STUDENT', 'B-PHONE_NUM', 'O', 'I-ID_NUM', 'I-URL_PERSONAL', 'B-USERNAME', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'B-EMAIL', 'B-ID_NUM', 'I-PHONE_NUM', 'B-NAME_STUDENT', 'B-URL_PERSONAL'}\n",
      "-------------------------\n",
      "label_counts: {'O': 4989794, 'B-NAME_STUDENT': 1365, 'I-NAME_STUDENT': 1096, 'B-URL_PERSONAL': 110, 'B-EMAIL': 39, 'B-ID_NUM': 78, 'I-URL_PERSONAL': 1, 'B-USERNAME': 6, 'B-PHONE_NUM': 6, 'I-PHONE_NUM': 15, 'B-STREET_ADDRESS': 2, 'I-STREET_ADDRESS': 20, 'I-ID_NUM': 1}\n"
     ]
    }
   ],
   "source": [
    "# EDA\n",
    "tranjs_path=\"dataset/train.json\"\n",
    "train_df = json.load(open(tranjs_path))\n",
    "print(f\"len(train_df):{len(train_df)}, train_df[0].keys(): {list(train_df[0].keys())}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "print(f\"train_df length:\", len(train_df))\n",
    "\n",
    "labels = set()\n",
    "label_counts = {}\n",
    "for i in range(len(train_df)):\n",
    "    labels.update(train_df[i]['labels'])\n",
    "    for label in train_df[i]['labels']:\n",
    "        if label in label_counts:\n",
    "            label_counts[label] += 1\n",
    "        else:\n",
    "            label_counts[label] = 1\n",
    "            \n",
    "print(f\"labels: {labels}\")\n",
    "print('-'*25)\n",
    "print(f\"label_counts: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # EDA\n",
    "# # display bar\n",
    "# import copy\n",
    "# display_label=copy.copy(label_counts)\n",
    "# del  display_label['O']\n",
    "# y =  [i for i in display_label.values()]\n",
    "\n",
    "# g=sns.barplot(x=list(display_label.keys()),y=y)\n",
    "# g.set_xticklabels(g.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['656',\n",
       "  'I-STREET_ADDRESS',\n",
       "  '\\n',\n",
       "  'I-STREET_ADDRESS',\n",
       "  'Joshuamouth',\n",
       "  'I-STREET_ADDRESS'],\n",
       " ['419',\n",
       "  'I-STREET_ADDRESS',\n",
       "  '\\n',\n",
       "  'I-STREET_ADDRESS',\n",
       "  'Andreahaven',\n",
       "  'I-STREET_ADDRESS']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_list=[]\n",
    "for i in train_df:\n",
    "    for j in range(0,len(i['tokens'])):\n",
    "        if i['tokens'][j].isspace() and i['labels'][j]!='O':\n",
    "            tmp_list.append([i['tokens'][j-1],i['labels'][j-1],i['tokens'][j],i['labels'][j],i['tokens'][j+1],i['labels'][j+1]])\n",
    "tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Thinking</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>innovation</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>reflexion</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992528</th>\n",
       "      <td>22687</td>\n",
       "      <td>process</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992529</th>\n",
       "      <td>22687</td>\n",
       "      <td>explained</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992530</th>\n",
       "      <td>22687</td>\n",
       "      <td>above</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992531</th>\n",
       "      <td>22687</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992532</th>\n",
       "      <td>22687</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4992533 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         document      tokens labels\n",
       "0               7      Design      O\n",
       "1               7    Thinking      O\n",
       "2               7         for      O\n",
       "3               7  innovation      O\n",
       "4               7   reflexion      O\n",
       "...           ...         ...    ...\n",
       "4992528     22687     process      O\n",
       "4992529     22687   explained      O\n",
       "4992530     22687       above      O\n",
       "4992531     22687           .      O\n",
       "4992532     22687        \\n\\n      O\n",
       "\n",
       "[4992533 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.DataFrame(train_df)\n",
    "df_train_tokens=[]\n",
    "for row in df_train.itertuples(index=False):\n",
    "    for i in range(len(row.tokens)):\n",
    "        df_train_tokens.append((row.document,row.tokens[i],row.labels[i]))\n",
    "df_train_tokens=pd.DataFrame(df_train_tokens,columns=['document','tokens','labels'])\n",
    "df_train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        document        tokens       labels\n",
      "150909      4381             (  B-PHONE_NUM\n",
      "150910      4381       320)202  I-PHONE_NUM\n",
      "150911      4381             -  I-PHONE_NUM\n",
      "150912      4381    0688x95843  I-PHONE_NUM\n",
      "179880      4777             (  B-PHONE_NUM\n",
      "179881      4777       223)392  I-PHONE_NUM\n",
      "179882      4777             -  I-PHONE_NUM\n",
      "179883      4777          2765  I-PHONE_NUM\n",
      "286835      6243             (  B-PHONE_NUM\n",
      "286836      6243       820)913  I-PHONE_NUM\n",
      "286837      6243             -  I-PHONE_NUM\n",
      "286838      6243      3241x894  I-PHONE_NUM\n",
      "287266      6243             (  B-PHONE_NUM\n",
      "287267      6243       820)913  I-PHONE_NUM\n",
      "287268      6243             -  I-PHONE_NUM\n",
      "287269      6243      3241x894  I-PHONE_NUM\n",
      "287293      6243             (  B-PHONE_NUM\n",
      "287294      6243       820)913  I-PHONE_NUM\n",
      "287295      6243             -  I-PHONE_NUM\n",
      "287296      6243      3241x894  I-PHONE_NUM\n",
      "861215      9854  410.526.1667  B-PHONE_NUM\n"
     ]
    }
   ],
   "source": [
    "# inspect specific label\n",
    "inspect=\"PHONE\"\n",
    "df_temp=df_train_tokens.loc[ df_train_tokens.labels.str.contains(inspect)]\n",
    "print(df_temp.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEA\n",
    "# print all unicode character\n",
    "# def has_unicode_codepoints(text):\n",
    "#     return any(ord(char) > 127 for char in text)\n",
    "# tmp_set=set()\n",
    "# pattern =re.compile(r\"[^\\u0000-\\u00ff]\")\n",
    "# for i in data:\n",
    "#     for j in i['tokens']:\n",
    "#         if has_unicode_codepoints(j):\n",
    "#             tmp_set.add(j)\n",
    "# for i in tmp_set:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp ./nltk_data/* ~/nltk_data/ -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [str(x[\"document\"]) for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "    \"labels\": [x[\"labels\"] for x in data]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测时，空缺值填'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# tmp_list=[]\n",
    "# for doc in data:\n",
    "#     full_text=doc['full_text']\n",
    "#     split_lines=full_text.split('\\n\\n')\n",
    "#     for i in split_lines:\n",
    "#         tmp_list.append(len(tokenizer(i)['input_ids']))\n",
    "# print(len(tmp_list))\n",
    "# print(max(tmp_list))\n",
    "# sns.displot(tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2bd686394b47579a10926cd95ca057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prepocessing data (num_proc=8):   0%|          | 0/9333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:failed to hit multiple times in a row,nothit:4,doc_id:11806\n",
      "Warning:failed to hit multiple times in a row,nothit:5,doc_id:11806\n",
      "Warning:failed to hit multiple times in a row,nothit:6,doc_id:11806\n",
      "Warning:failed to hit multiple times in a row,nothit:7,doc_id:11806\n",
      "Warning:failed to hit multiple times in a row,nothit:8,doc_id:11806\n",
      "Warning:failed to hit multiple times in a row,nothit:4,doc_id:9783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['full_text', 'document', 'tokens', 'trailing_whitespace', 'labels', 'splited_sens', 'berttokenpos2orgtokenpos', 'bertlabels', 'berttokenids', 'berttokenmask', 'berttokentoken_type_ids'],\n",
       "    num_rows: 9333\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果多次没击中，大概率有问题\n",
    "preprocesssed_ds=ds.map(preprocesss, fn_kwargs={'tokenizer':tokenizer,'label2id':label2id,'if_train':True},num_proc=8,desc=\"prepocessing data\")\n",
    "preprocesssed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(preprocesssed_ds[0]['berttokenids']) == len(preprocesssed_ds[0]['splited_sens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 323, 2], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_str=\".\"\n",
    "tmp_res=tokenizer(tmp_str)\n",
    "tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '▁.', '[SEP]']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tmp_res['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean     71.305686\n",
       "max     936.000000\n",
       "std     104.360657\n",
       "Name: berttokenids, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rebuid dataset \n",
    "tmp_pd=expanddataset(preprocesssed_ds)\n",
    "tmp_pd['berttokenids'].str.len().agg(['mean','max','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['full_text', 'document', 'tokens', 'trailing_whitespace', 'labels', 'splited_sens', 'berttokenpos2orgtokenpos', 'berttokenids', 'berttokenmask', 'berttokentoken_type_ids', 'bertlabels'],\n",
      "    num_rows: 70386\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "full_ds=Dataset.from_pandas(tmp_pd)\n",
    "print(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_len=int(0.9*len(full_ds)) # 长度分割，兼容老版本torch\n",
    "train_dataset, val_dataset = random_split(full_ds, [train_len,len(full_ds)-train_len],generator=torch.Generator().manual_seed(Config.seed)) #TODO:改变比例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset len:63347\n",
      "vail dataset len:7039\n"
     ]
    }
   ],
   "source": [
    "print(f\"train dataset len:{len(train_dataset)}\")\n",
    "print(f\"vail dataset len:{len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = Collate(tokenizer=tokenizer)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=Config.batch_size,pin_memory=True,collate_fn=data_collator,shuffle=True)\n",
    "val_dataloader=DataLoader(val_dataset,batch_size=Config.batch_size,pin_memory=True,collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from model1 import NLPModel\n",
    "model=NLPModel(id2label,label2id,Config.model_name).to(device)\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from seqeval.metrics import recall_score, precision_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "optimizer= AdamW(model.parameters(),lr=Config.lr,weight_decay=Config.weight_decay)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "valstep = len(train_dataloader) //Config.evaltimes\n",
    "\n",
    "num_train_steps = int(len(train_dataloader) * Config.epochs)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=Config.num_warmup_steps, num_training_steps=num_train_steps, \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions,traget, all_labels):\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, traget)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, traget)\n",
    "    ]\n",
    "    \n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "    \n",
    "    results = {\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1_score\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def validation(val_dataloader,model):\n",
    "    model.eval()\n",
    "\n",
    "    allprecitions=[]\n",
    "    alltargets=[]\n",
    "    for step,dataset in enumerate(tqdm(val_dataloader)):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ids = dataset[\"ids\"].to(device,non_blocking=True)\n",
    "            mask = dataset[\"mask\"].to(device,non_blocking=True)\n",
    "            targets = dataset[\"targets\"].to(device,non_blocking=True)\n",
    "            tokentype = dataset[\"type_ids\"].to(device,non_blocking=True)\n",
    "            logit,loss = model(ids,mask, tokentype,targets)\n",
    "\n",
    "            targets=targets.view(-1)\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "\n",
    "            allprecitions.append(np.argmax(logit, axis=1))\n",
    "            alltargets.append(targets)\n",
    "        del ids,mask,targets,tokentype,loss,logit\n",
    "\n",
    "    score=compute_metrics(allprecitions,alltargets,all_labels)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "start_time=time.time()\n",
    "bestscore=0\n",
    "epoch=0\n",
    "\n",
    "if Config.resume_train_epoch:\n",
    "    checkpoint = torch.load(Config.workdir+f\"//checkpoint//checkpoint{Config.resume_train_epoch}.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    bestscore=checkpoint['bestscore']\n",
    "    model.train()\n",
    "    print(f\"start epoch:{epoch}\")\n",
    "\n",
    "while epoch <Config.epochs:\n",
    "    losses=[] \n",
    "    for step,dataset in enumerate(tqdm(train_dataloader)):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            ids = dataset[\"ids\"].to(device,non_blocking=True)\n",
    "            mask = dataset[\"mask\"].to(device,non_blocking=True)\n",
    "            targets = dataset[\"targets\"].to(device,non_blocking=True)\n",
    "            tokentype = dataset[\"type_ids\"].to(device,non_blocking=True)\n",
    "\n",
    "            logit,loss = model(ids,mask, tokentype,targets)\n",
    "            loss/= Config.accumulation_steps\n",
    "\n",
    "        # Accumulates scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "        # losses.append(loss.item())\n",
    "        \n",
    "        if (step+1) % Config.accumulation_steps ==0:\n",
    "            scaler.step(optimizer)\n",
    "            scale = scaler.get_scale()\n",
    "            # Updates the scale for next iteration.\n",
    "            scaler.update()\n",
    "            skip_lr_sched = (scale != scaler.get_scale())\n",
    "            if not skip_lr_sched:\n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # validation\n",
    "        if epoch>0 and (step+1) % valstep == 0:\n",
    "            scores=validation(val_dataloader,model)\n",
    "            print(\"Validation:Epoch{} Step [{}/{}] Loss: {:.3f} Time: {:.1f} Metric: {}\".format(epoch,step, len(train_dataloader)-1, loss.item(), time.time()-start_time,scores))\n",
    "            if scores['f1']>bestscore:\n",
    "                print(f\"Best score is {bestscore} → {scores['f1']}. Saving model\")\n",
    "                torch.save(model.state_dict(), os.path.join(Config.modelsavepath,\"model_seed{}_score{:.3f}.pth\".format(Config.seed,bestscore)))\n",
    "                bestscore=scores['f1']\n",
    "            else:\n",
    "                print(\"no improvement, bestf1 is {:.3f} score is {}\".format(bestscore,scores))\n",
    "                # report loss\n",
    "        if (step+1) % Config.logging_steps ==0:\n",
    "            # print(\"Logging: Step [{}/{}] Loss: {:.3f} Time: {:.1f}\".format(step, len(train_dataloader)-1, loss.item(), time.time()-start_time))\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        del ids,mask,targets,tokentype,loss,logit\n",
    "\n",
    "    epoch+=1\n",
    "    # checkpoint\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'bestscore':bestscore,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict':scheduler.state_dict(),\n",
    "            'scaler_state_dict':scaler.state_dict()\n",
    "            }, Config.workdir+\"//checkpoint/checkpoint{}.pth\".format(epoch))\n",
    "\n",
    "tokenizer.save_pretrained(Config.modelsavepath)\n",
    "os.system(\"/usr/bin/shutdown\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
