{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "# output = result.stdout\n",
    "# for line in output.splitlines():\n",
    "#     if '=' in line:\n",
    "#         var, value = line.split('=', 1)\n",
    "#         os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cfg import *\n",
    "from util import *\n",
    "\n",
    "##### main\n",
    "data = json.load(open(Config.data_path+\"train.json\"))\n",
    "data_pd=pd.DataFrame(data)\n",
    "len(data_pd[data_pd['tokens'].apply(len)>2024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/z1933/workplace/machinelearning/PII-Data-Detection//modelsave/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.modelsavepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document                                                           21720\n",
       "full_text              In this assignment, a reflective report will b...\n",
       "tokens                 [In, this, assignment, ,, a, reflective, repor...\n",
       "trailing_whitespace    [True, True, False, True, True, True, True, Tr...\n",
       "labels                 [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "Name: 6365, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd.iloc[data_pd.full_text.str.len().idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z1933/miniconda3/envs/d2l/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
    "tokenized_input =tokenizer(data_pd.iloc[data_pd.full_text.str.len().idxmax()]['full_text'],return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2900])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "len(tokenized_input[\"input_ids\"])\n",
    "tokenized_input[\"input_ids\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_labels:['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'B-EMAIL',\n",
       " 1: 'B-ID_NUM',\n",
       " 2: 'B-NAME_STUDENT',\n",
       " 3: 'B-PHONE_NUM',\n",
       " 4: 'B-STREET_ADDRESS',\n",
       " 5: 'B-URL_PERSONAL',\n",
       " 6: 'B-USERNAME',\n",
       " 7: 'I-ID_NUM',\n",
       " 8: 'I-NAME_STUDENT',\n",
       " 9: 'I-PHONE_NUM',\n",
       " 10: 'I-STREET_ADDRESS',\n",
       " 11: 'I-URL_PERSONAL',\n",
       " 12: 'O'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "#将所有数据的labels连接在一起,然后查重,转成list的格式,然后从小到大排序\n",
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "print(f\"all_labels:{all_labels}\")\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "#这个是{id:label},上面是{label:id}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "# Open a file for writing\n",
    "with open(Config.modelsavepath+\"//idlabel.json\", \"w\") as f:\n",
    "    # Write the map to the file in JSON format\n",
    "    json.dump({'label2id':label2id,'id2label':id2label}, f)\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================\n",
    "Layer (type:depth-idx)                                                 Output Shape              Param #\n",
    "========================================================================================================================\n",
    "NLPModel                                                               [1, 100, 13]              --\n",
    "├─DebertaV2ForTokenClassification: 1-1                                 [1, 100, 13]              --\n",
    "│    └─DebertaV2Model: 2-1                                             [1, 100, 1024]            --\n",
    "│    │    └─DebertaV2Embeddings: 3-1                                   [1, 100, 1024]            131,176,448\n",
    "│    │    └─DebertaV2Encoder: 3-2                                      [1, 100, 1024]            302,835,712\n",
    "│    └─Dropout: 2-2                                                    [1, 100, 1024]            --\n",
    "│    └─Linear: 2-3                                                     [1, 100, 13]              13,325\n",
    "========================================================================================================================\n",
    "Total params: 434,025,485\n",
    "Trainable params: 434,025,485\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (M): 484.93\n",
    "========================================================================================================================\n",
    "Input size (MB): 0.00\n",
    "Forward/backward pass size (MB): 423.44\n",
    "Params size (MB): 1734.00\n",
    "Estimated Total Size (MB): 2157.45\n",
    "======================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_df):6807, train_df[0].keys(): ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels']\n",
      "--------------------------------------------------\n",
      "train_df length: 6807\n",
      "labels: {'B-USERNAME', 'I-NAME_STUDENT', 'I-STREET_ADDRESS', 'I-ID_NUM', 'I-URL_PERSONAL', 'B-URL_PERSONAL', 'O', 'B-ID_NUM', 'B-STREET_ADDRESS', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-EMAIL', 'I-PHONE_NUM'}\n",
      "-------------------------\n",
      "label_counts: {'O': 4989794, 'B-NAME_STUDENT': 1365, 'I-NAME_STUDENT': 1096, 'B-URL_PERSONAL': 110, 'B-EMAIL': 39, 'B-ID_NUM': 78, 'I-URL_PERSONAL': 1, 'B-USERNAME': 6, 'B-PHONE_NUM': 6, 'I-PHONE_NUM': 15, 'B-STREET_ADDRESS': 2, 'I-STREET_ADDRESS': 20, 'I-ID_NUM': 1}\n"
     ]
    }
   ],
   "source": [
    "# EDA\n",
    "tranjs_path=\"dataset/train.json\"\n",
    "train_df = json.load(open(tranjs_path))\n",
    "print(f\"len(train_df):{len(train_df)}, train_df[0].keys(): {list(train_df[0].keys())}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "print(f\"train_df length:\", len(train_df))\n",
    "\n",
    "labels = set()\n",
    "label_counts = {}\n",
    "for i in range(len(train_df)):\n",
    "    labels.update(train_df[i]['labels'])\n",
    "    for label in train_df[i]['labels']:\n",
    "        if label in label_counts:\n",
    "            label_counts[label] += 1\n",
    "        else:\n",
    "            label_counts[label] = 1\n",
    "            \n",
    "print(f\"labels: {labels}\")\n",
    "print('-'*25)\n",
    "print(f\"label_counts: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # EDA\n",
    "# # display bar\n",
    "# import copy\n",
    "# display_label=copy.copy(label_counts)\n",
    "# del  display_label['O']\n",
    "# y =  [i for i in display_label.values()]\n",
    "\n",
    "# g=sns.barplot(x=list(display_label.keys()),y=y)\n",
    "# g.set_xticklabels(g.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['656',\n",
       "  'I-STREET_ADDRESS',\n",
       "  '\\n',\n",
       "  'I-STREET_ADDRESS',\n",
       "  'Joshuamouth',\n",
       "  'I-STREET_ADDRESS'],\n",
       " ['419',\n",
       "  'I-STREET_ADDRESS',\n",
       "  '\\n',\n",
       "  'I-STREET_ADDRESS',\n",
       "  'Andreahaven',\n",
       "  'I-STREET_ADDRESS']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_list=[]\n",
    "for i in train_df:\n",
    "    for j in range(0,len(i['tokens'])):\n",
    "        if i['tokens'][j].isspace() and i['labels'][j]!='O':\n",
    "            tmp_list.append([i['tokens'][j-1],i['labels'][j-1],i['tokens'][j],i['labels'][j],i['tokens'][j+1],i['labels'][j+1]])\n",
    "tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Design</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Thinking</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>innovation</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>reflexion</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992528</th>\n",
       "      <td>22687</td>\n",
       "      <td>process</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992529</th>\n",
       "      <td>22687</td>\n",
       "      <td>explained</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992530</th>\n",
       "      <td>22687</td>\n",
       "      <td>above</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992531</th>\n",
       "      <td>22687</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992532</th>\n",
       "      <td>22687</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4992533 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         document      tokens labels\n",
       "0               7      Design      O\n",
       "1               7    Thinking      O\n",
       "2               7         for      O\n",
       "3               7  innovation      O\n",
       "4               7   reflexion      O\n",
       "...           ...         ...    ...\n",
       "4992528     22687     process      O\n",
       "4992529     22687   explained      O\n",
       "4992530     22687       above      O\n",
       "4992531     22687           .      O\n",
       "4992532     22687        \\n\\n      O\n",
       "\n",
       "[4992533 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.DataFrame(train_df)\n",
    "df_train_tokens=[]\n",
    "for row in df_train.itertuples(index=False):\n",
    "    for i in range(len(row.tokens)):\n",
    "        df_train_tokens.append((row.document,row.tokens[i],row.labels[i]))\n",
    "df_train_tokens=pd.DataFrame(df_train_tokens,columns=['document','tokens','labels'])\n",
    "df_train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        document        tokens       labels\n",
      "150909      4381             (  B-PHONE_NUM\n",
      "150910      4381       320)202  I-PHONE_NUM\n",
      "150911      4381             -  I-PHONE_NUM\n",
      "150912      4381    0688x95843  I-PHONE_NUM\n",
      "179880      4777             (  B-PHONE_NUM\n",
      "179881      4777       223)392  I-PHONE_NUM\n",
      "179882      4777             -  I-PHONE_NUM\n",
      "179883      4777          2765  I-PHONE_NUM\n",
      "286835      6243             (  B-PHONE_NUM\n",
      "286836      6243       820)913  I-PHONE_NUM\n",
      "286837      6243             -  I-PHONE_NUM\n",
      "286838      6243      3241x894  I-PHONE_NUM\n",
      "287266      6243             (  B-PHONE_NUM\n",
      "287267      6243       820)913  I-PHONE_NUM\n",
      "287268      6243             -  I-PHONE_NUM\n",
      "287269      6243      3241x894  I-PHONE_NUM\n",
      "287293      6243             (  B-PHONE_NUM\n",
      "287294      6243       820)913  I-PHONE_NUM\n",
      "287295      6243             -  I-PHONE_NUM\n",
      "287296      6243      3241x894  I-PHONE_NUM\n",
      "861215      9854  410.526.1667  B-PHONE_NUM\n"
     ]
    }
   ],
   "source": [
    "# inspect specific label\n",
    "inspect=\"PHONE\"\n",
    "df_temp=df_train_tokens.loc[ df_train_tokens.labels.str.contains(inspect)]\n",
    "print(df_temp.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>full_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>4381</td>\n",
       "      <td>WRITING CENTRE  Level 3 East, Hub Central  Nor...</td>\n",
       "      <td>[WRITING, CENTRE,  , Level, 3, East, ,, Hub, C...</td>\n",
       "      <td>[True, True, False, True, True, False, True, T...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     document                                          full_text  \\\n",
       "185      4381  WRITING CENTRE  Level 3 East, Hub Central  Nor...   \n",
       "\n",
       "                                                tokens  \\\n",
       "185  [WRITING, CENTRE,  , Level, 3, East, ,, Hub, C...   \n",
       "\n",
       "                                   trailing_whitespace  \\\n",
       "185  [True, True, False, True, True, False, True, T...   \n",
       "\n",
       "                                                labels  \n",
       "185  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.loc[input_data['document']==4381]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEA\n",
    "# print all unicode character\n",
    "# def has_unicode_codepoints(text):\n",
    "#     return any(ord(char) > 127 for char in text)\n",
    "# tmp_set=set()\n",
    "# pattern =re.compile(r\"[^\\u0000-\\u00ff]\")\n",
    "# for i in data:\n",
    "#     for j in i['tokens']:\n",
    "#         if has_unicode_codepoints(j):\n",
    "#             tmp_set.add(j)\n",
    "# for i in tmp_set:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp ./nltk_data/* ~/nltk_data/ -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [x[\"document\"] for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "    \"labels\": [x[\"labels\"] for x in data]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测时，空缺值填'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# tmp_list=[]\n",
    "# for doc in data:\n",
    "#     full_text=doc['full_text']\n",
    "#     split_lines=full_text.split('\\n\\n')\n",
    "#     for i in split_lines:\n",
    "#         tmp_list.append(len(tokenizer(i)['input_ids']))\n",
    "# print(len(tmp_list))\n",
    "# print(max(tmp_list))\n",
    "# sns.displot(tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1914f409244d8592fa89269c3c8673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prepocessing data (num_proc=8):   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:failed to hit multiple times in a row,nothit:4,doc_id:9783\n",
      "Warning:failed to hit multiple times in a row,nothit:4,doc_id:11806\n",
      "Warning:failed to hit multiple times in a row,nothit:5,doc_id:11806\n",
      "Warning:failed to hit multiple times in a row,nothit:6,doc_id:11806\n",
      "Warning:failed to hit multiple times in a row,nothit:7,doc_id:11806\n",
      "Warning:failed to hit multiple times in a row,nothit:8,doc_id:11806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['full_text', 'document', 'tokens', 'trailing_whitespace', 'labels', 'splited_sens', 'berttokenpos2orgtokenpos', 'bertlabels', 'berttokenids', 'berttokenmask', 'berttokentoken_type_ids'],\n",
       "    num_rows: 6807\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果多次没击中，大概率有问题\n",
    "preprocesssed_ds=ds.map(preprocesss, fn_kwargs={'tokenizer':tokenizer,'label2id':label2id,'if_train':True},num_proc=8,desc=\"prepocessing data\")\n",
    "preprocesssed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(preprocesssed_ds[0]['berttokenids']) == len(preprocesssed_ds[0]['splited_sens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 323, 2], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_str=\".\"\n",
    "tmp_res=tokenizer(tmp_str)\n",
    "tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '▁.', '[SEP]']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tmp_res['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean      44.367558\n",
       "max     1036.000000\n",
       "std       73.714812\n",
       "Name: berttokenids, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rebuid dataset \n",
    "tmp_pd=expanddataset(preprocesssed_ds)\n",
    "tmp_pd['berttokenids'].str.len().agg(['mean','max','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['full_text', 'document', 'tokens', 'trailing_whitespace', 'labels', 'splited_sens', 'berttokenpos2orgtokenpos', 'berttokenids', 'berttokenmask', 'berttokentoken_type_ids', 'bertlabels'],\n",
       "    num_rows: 110943\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ds=Dataset.from_pandas(tmp_pd)\n",
    "full_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_dataset, val_dataset = random_split(full_ds, [0.8, 0.2],generator=torch.Generator().manual_seed(Config.seed)) #TODO:改变比例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88755\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = Collate(tokenizer=tokenizer)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=Config.batch_size,pin_memory=True,collate_fn=data_collator,shuffle=True)\n",
    "val_dataloader=DataLoader(val_dataset,batch_size=Config.batch_size,pin_memory=True,collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from model1 import NLPModel\n",
    "model=NLPModel(id2label,label2id).to(device)\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from seqeval.metrics import recall_score, precision_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "optimizer= AdamW(model.parameters(),lr=Config.lr,weight_decay=Config.weight_decay)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "valstep = len(train_dataloader) //Config.evaltimes\n",
    "\n",
    "num_train_steps = int(len(train_dataloader) * Config.epochs)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=Config.num_warmup_steps, num_training_steps=num_train_steps, \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions,traget, all_labels):\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, traget)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, traget)\n",
    "    ]\n",
    "    \n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "    \n",
    "    results = {\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1_score\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def validation(val_dataloader,model):\n",
    "    model.eval()\n",
    "\n",
    "    allprecitions=[]\n",
    "    alltargets=[]\n",
    "    for step,dataset in enumerate(tqdm(val_dataloader)):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ids = dataset[\"ids\"].to(device,non_blocking=True)\n",
    "            mask = dataset[\"mask\"].to(device,non_blocking=True)\n",
    "            targets = dataset[\"targets\"].to(device,non_blocking=True)\n",
    "            tokentype = dataset[\"type_ids\"].to(device,non_blocking=True)\n",
    "            logit,loss = model(ids,mask, tokentype,targets)\n",
    "\n",
    "            targets=targets.view(-1)\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "\n",
    "            allprecitions.append(np.argmax(logit, axis=1))\n",
    "            alltargets.append(targets)\n",
    "        del ids,mask,targets,tokentype,loss,logit\n",
    "\n",
    "    score=compute_metrics(allprecitions,alltargets,all_labels)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 67/22189 [00:10<1:00:21,  6.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39maccumulation_steps\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Accumulates scaled gradients.\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# losses.append(loss.item())\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m Config\u001b[38;5;241m.\u001b[39maccumulation_steps \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "start_time=time.time()\n",
    "bestscore=0\n",
    "epoch=0\n",
    "\n",
    "if Config.resume_train_epoch:\n",
    "    checkpoint = torch.load(Config.workdir+f\"//checkpoint//checkpoint{Config.resume_train_epoch}.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    bestscore=checkpoint['bestscore']\n",
    "    model.train()\n",
    "    print(f\"start epoch:{epoch}\")\n",
    "\n",
    "while epoch <Config.epochs:\n",
    "    losses=[] \n",
    "    for step,dataset in enumerate(tqdm(train_dataloader)):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            ids = dataset[\"ids\"].to(device,non_blocking=True)\n",
    "            mask = dataset[\"mask\"].to(device,non_blocking=True)\n",
    "            targets = dataset[\"targets\"].to(device,non_blocking=True)\n",
    "            tokentype = dataset[\"type_ids\"].to(device,non_blocking=True)\n",
    "\n",
    "            logit,loss = model(ids,mask, tokentype,targets)\n",
    "            loss/= Config.accumulation_steps\n",
    "\n",
    "        # Accumulates scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "        # losses.append(loss.item())\n",
    "        \n",
    "        if (step+1) % Config.accumulation_steps ==0:\n",
    "            scaler.step(optimizer)\n",
    "            scale = scaler.get_scale()\n",
    "            # Updates the scale for next iteration.\n",
    "            scaler.update()\n",
    "            skip_lr_sched = (scale != scaler.get_scale())\n",
    "            if not skip_lr_sched:\n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # validation\n",
    "        if epoch>0 and (step+1) % valstep == 0:\n",
    "            scores=validation(val_dataloader,model)\n",
    "            print(\"Validation:Epoch{} Step [{}/{}] Loss: {:.3f} Time: {:.1f} Metric: {}\".format(epoch,step, len(train_dataloader)-1, loss.item(), time.time()-start_time,scores))\n",
    "            if scores['f1']>bestscore:\n",
    "                print(f\"Best score is {bestscore} → {scores['f1']}. Saving model\")\n",
    "                torch.save(model.state_dict(), os.path.join(Config.modelsavepath,\"model_seed{}_score{:.3f}.pth\".format(Config.seed,bestscore)))\n",
    "                bestscore=scores['f1']\n",
    "            else:\n",
    "                print(\"no improvement, bestf1 is {:.3f} score is {}\".format(bestscore,scores))\n",
    "                # report loss\n",
    "        if (step+1) % Config.logging_steps ==0:\n",
    "            # print(\"Logging: Step [{}/{}] Loss: {:.3f} Time: {:.1f}\".format(step, len(train_dataloader)-1, loss.item(), time.time()-start_time))\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        del ids,mask,targets,tokentype,loss,logit\n",
    "\n",
    "    epoch+=1\n",
    "    # checkpoint\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'bestscore':bestscore,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict':scheduler.state_dict(),\n",
    "            'scaler_state_dict':scaler.state_dict()\n",
    "            }, Config.workdir+\"//checkpoint/checkpoint{}.pth\".format(epoch))\n",
    "\n",
    "os.system(\"/usr/bin/shutdown\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
