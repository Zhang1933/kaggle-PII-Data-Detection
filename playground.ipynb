{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "# output = result.stdout\n",
    "# for line in output.splitlines():\n",
    "#     if '=' in line:\n",
    "#         var, value = line.split('=', 1)\n",
    "#         os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir -p modelsave checkpoint/\n",
    "# %pip install --force-reinstall /home/z1933/workplace/machinelearning/PII-Data-Detection/unidecode/Unidecode-1.3.8-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/z1933/workplace/machinelearning/PII-Data-Detection//modelsave/'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cfg import *\n",
    "from util import *\n",
    "Config.modelsavepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original datapoints:  6807\n",
      "external datapoints:  2355\n",
      "moredata datapoints:  2000\n",
      "combined:  7254\n"
     ]
    }
   ],
   "source": [
    "##### main\n",
    "data = json.load(open(Config.data_path+\"/train.json\"))\n",
    "\n",
    "# downsampling of negative examples\n",
    "p=[] # positive samples (contain relevant labels)\n",
    "n=[] # negative samples (presumably contain entities that are possibly wrongly classified as entity)\n",
    "for d in data:\n",
    "    if any(np.array(d[\"labels\"]) != \"O\"): p.append(d)\n",
    "    else: n.append(d)\n",
    "print(\"original datapoints: \", len(data))\n",
    "\n",
    "external = json.load(open(Config.data_path+\"/mixtral-8x7b-v1.json\"))\n",
    "print(\"external datapoints: \", len(external))\n",
    "\n",
    "moredata = json.load(open(Config.data_path+\"/moredata_dataset_fixed.json\"))\n",
    "print(\"moredata datapoints: \", len(moredata))\n",
    "\n",
    "data = moredata+external+p+n[:len(n)//3]\n",
    "print(\"combined: \", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd=pd.DataFrame(data)\n",
    "len(data_pd[data_pd['tokens'].apply(len)>2024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z1933/miniconda3/envs/d2l/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
    "tokenized_input =tokenizer(data_pd.iloc[data_pd.full_text.str.len().idxmax()]['full_text'],return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2831])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokenized_input[\"input_ids\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_labels:['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'B-EMAIL',\n",
       " 1: 'B-ID_NUM',\n",
       " 2: 'B-NAME_STUDENT',\n",
       " 3: 'B-PHONE_NUM',\n",
       " 4: 'B-STREET_ADDRESS',\n",
       " 5: 'B-URL_PERSONAL',\n",
       " 6: 'B-USERNAME',\n",
       " 7: 'I-ID_NUM',\n",
       " 8: 'I-NAME_STUDENT',\n",
       " 9: 'I-PHONE_NUM',\n",
       " 10: 'I-STREET_ADDRESS',\n",
       " 11: 'I-URL_PERSONAL',\n",
       " 12: 'O'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "#将所有数据的labels连接在一起,然后查重,转成list的格式,然后从小到大排序\n",
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "print(f\"all_labels:{all_labels}\")\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "#这个是{id:label},上面是{label:id}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "# Open a file for writing\n",
    "with open(Config.modelsavepath+\"//idlabel.json\", \"w\") as f:\n",
    "    # Write the map to the file in JSON format\n",
    "    json.dump({'label2id':label2id,'id2label':id2label}, f)\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_df):6807, train_df[0].keys(): ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels']\n",
      "--------------------------------------------------\n",
      "train_df length: 6807\n",
      "labels: {'B-ID_NUM', 'I-STREET_ADDRESS', 'B-STREET_ADDRESS', 'I-PHONE_NUM', 'I-ID_NUM', 'O', 'I-NAME_STUDENT', 'I-URL_PERSONAL', 'B-PHONE_NUM', 'B-URL_PERSONAL', 'B-NAME_STUDENT', 'B-USERNAME', 'B-EMAIL'}\n",
      "-------------------------\n",
      "label_counts: {'O': 4989794, 'B-NAME_STUDENT': 1365, 'I-NAME_STUDENT': 1096, 'B-URL_PERSONAL': 110, 'B-EMAIL': 39, 'B-ID_NUM': 78, 'I-URL_PERSONAL': 1, 'B-USERNAME': 6, 'B-PHONE_NUM': 6, 'I-PHONE_NUM': 15, 'B-STREET_ADDRESS': 2, 'I-STREET_ADDRESS': 20, 'I-ID_NUM': 1}\n"
     ]
    }
   ],
   "source": [
    "# EDA\n",
    "tranjs_path=\"dataset/train.json\"\n",
    "train_df = json.load(open(tranjs_path))\n",
    "print(f\"len(train_df):{len(train_df)}, train_df[0].keys(): {list(train_df[0].keys())}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "print(f\"train_df length:\", len(train_df))\n",
    "\n",
    "labels = set()\n",
    "label_counts = {}\n",
    "for i in range(len(train_df)):\n",
    "    labels.update(train_df[i]['labels'])\n",
    "    for label in train_df[i]['labels']:\n",
    "        if label in label_counts:\n",
    "            label_counts[label] += 1\n",
    "        else:\n",
    "            label_counts[label] = 1\n",
    "            \n",
    "print(f\"labels: {labels}\")\n",
    "print('-'*25)\n",
    "print(f\"label_counts: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # EDA\n",
    "# # display bar\n",
    "# import copy\n",
    "# display_label=copy.copy(label_counts)\n",
    "# del  display_label['O']\n",
    "# y =  [i for i in display_label.values()]\n",
    "\n",
    "# g=sns.barplot(x=list(display_label.keys()),y=y)\n",
    "# g.set_xticklabels(g.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['656',\n",
       "  'I-STREET_ADDRESS',\n",
       "  '\\n',\n",
       "  'I-STREET_ADDRESS',\n",
       "  'Joshuamouth',\n",
       "  'I-STREET_ADDRESS'],\n",
       " ['419',\n",
       "  'I-STREET_ADDRESS',\n",
       "  '\\n',\n",
       "  'I-STREET_ADDRESS',\n",
       "  'Andreahaven',\n",
       "  'I-STREET_ADDRESS']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_list=[]\n",
    "for i in train_df:\n",
    "    for j in range(0,len(i['tokens'])):\n",
    "        if i['tokens'][j].isspace() and i['labels'][j]!='O':\n",
    "            tmp_list.append([i['tokens'][j-1],i['labels'][j-1],i['tokens'][j],i['labels'][j],i['tokens'][j+1],i['labels'][j+1]])\n",
    "tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.DataFrame(train_df)\n",
    "df_train_tokens=[]\n",
    "for row in df_train.itertuples(index=False):\n",
    "    for i in range(len(row.tokens)):\n",
    "        df_train_tokens.append((row.document,row.tokens[i],row.labels[i],row.trailing_whitespace[i]))\n",
    "df_train_tokens=pd.DataFrame(df_train_tokens,columns=['document','tokens','labels','trailing_whitespace'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>861205</th>\n",
       "      <td>9854</td>\n",
       "      <td>\\n</td>\n",
       "      <td>I-STREET_ADDRESS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445336</th>\n",
       "      <td>11442</td>\n",
       "      <td>\\n</td>\n",
       "      <td>I-STREET_ADDRESS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         document tokens            labels  trailing_whitespace\n",
       "861205       9854     \\n  I-STREET_ADDRESS                False\n",
       "1445336     11442     \\n  I-STREET_ADDRESS                False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_tokens.loc[df_train_tokens['tokens']=='\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document                           9854\n",
       "tokens                      Joshuamouth\n",
       "labels                 I-STREET_ADDRESS\n",
       "trailing_whitespace               False\n",
       "Name: 861206, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_tokens.iloc[861206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        document        tokens       labels  trailing_whitespace\n",
      "150909      4381             (  B-PHONE_NUM                False\n",
      "150910      4381       320)202  I-PHONE_NUM                False\n",
      "150911      4381             -  I-PHONE_NUM                False\n",
      "150912      4381    0688x95843  I-PHONE_NUM                 True\n",
      "179880      4777             (  B-PHONE_NUM                False\n",
      "179881      4777       223)392  I-PHONE_NUM                False\n",
      "179882      4777             -  I-PHONE_NUM                False\n",
      "179883      4777          2765  I-PHONE_NUM                False\n",
      "286835      6243             (  B-PHONE_NUM                False\n",
      "286836      6243       820)913  I-PHONE_NUM                False\n",
      "286837      6243             -  I-PHONE_NUM                False\n",
      "286838      6243      3241x894  I-PHONE_NUM                False\n",
      "287266      6243             (  B-PHONE_NUM                False\n",
      "287267      6243       820)913  I-PHONE_NUM                False\n",
      "287268      6243             -  I-PHONE_NUM                False\n",
      "287269      6243      3241x894  I-PHONE_NUM                False\n",
      "287293      6243             (  B-PHONE_NUM                False\n",
      "287294      6243       820)913  I-PHONE_NUM                False\n",
      "287295      6243             -  I-PHONE_NUM                False\n",
      "287296      6243      3241x894  I-PHONE_NUM                False\n",
      "861215      9854  410.526.1667  B-PHONE_NUM                 True\n"
     ]
    }
   ],
   "source": [
    "# inspect specific label\n",
    "inspect=\"PHONE\"\n",
    "df_temp=df_train_tokens.loc[ df_train_tokens.labels.str.contains(inspect)]\n",
    "print(df_temp.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEA\n",
    "# print all unicode character\n",
    "# def has_unicode_codepoints(text):\n",
    "#     return any(ord(char) > 127 for char in text)\n",
    "# tmp_set=set()\n",
    "# pattern =re.compile(r\"[^\\u0000-\\u00ff]\")\n",
    "# for i in data:\n",
    "#     for j in i['tokens']:\n",
    "#         if has_unicode_codepoints(j):\n",
    "#             tmp_set.add(j)\n",
    "# for i in tmp_set:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp ./nltk_data/* ~/nltk_data/ -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [str(x[\"document\"]) for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "    \"labels\": [x[\"labels\"] for x in data]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测时，空缺值填'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# tmp_list=[]\n",
    "# for doc in data:\n",
    "#     full_text=doc['full_text']\n",
    "#     split_lines=full_text.split('\\n\\n')\n",
    "#     for i in split_lines:\n",
    "#         tmp_list.append(len(tokenizer(i)['input_ids']))\n",
    "# print(len(tmp_list))\n",
    "# print(max(tmp_list))\n",
    "# sns.displot(tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacaeb15d32b4d80b3f7f91fc597fd08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prepocessing data (num_proc=10):   0%|          | 0/7254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['full_text', 'document', 'tokens', 'trailing_whitespace', 'labels', 'bertlabels', 'berttokenids', 'berttokenmask', 'berttokentoken_type_ids'],\n",
       "    num_rows: 7254\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据'\\n\\n'分token\n",
    "preprocesssed_ds=ds.map(train_preprocesss, fn_kwargs={'tokenizer':tokenizer,'label2id':label2id},num_proc=10,desc=\"prepocessing data\")\n",
    "preprocesssed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 323, 2], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_str=\".\"\n",
    "tmp_res=tokenizer(tmp_str)\n",
    "tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '▁.', '[SEP]']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tmp_res['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean     52.748827\n",
      "max     933.000000\n",
      "std      56.812397\n",
      "min       3.000000\n",
      "Name: berttokenids, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>document</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespace</th>\n",
       "      <th>labels</th>\n",
       "      <th>berttokenids</th>\n",
       "      <th>berttokenmask</th>\n",
       "      <th>berttokentoken_type_ids</th>\n",
       "      <th>bertlabels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15384</th>\n",
       "      <td>Charles-Edouard Siegel | c-siegel@outlook.com;...</td>\n",
       "      <td>wcyzegeled</td>\n",
       "      <td>[Charles, -, Edouard, Siegel, |, c-siegel@outl...</td>\n",
       "      <td>[False, False, True, True, True, False, True, ...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, I-NAME_STUDEN...</td>\n",
       "      <td>[1, 5736, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[12, 12, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16630</th>\n",
       "      <td>Domenico Childers - 26973 Jennifer Ford\\nJimmy...</td>\n",
       "      <td>kvyrxtluau</td>\n",
       "      <td>[Domenico, Childers, -, 26973, Jennifer, Ford,...</td>\n",
       "      <td>[True, True, True, True, True, False, False, F...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, B-STREET_A...</td>\n",
       "      <td>[1, 10766, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[12, 12, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17711</th>\n",
       "      <td>Johanny Arnold | PSC 3616, Box 9945\\nAPO AE 52...</td>\n",
       "      <td>cyherfmagt</td>\n",
       "      <td>[Johanny, Arnold, |, PSC, 3616, ,, Box, 9945, ...</td>\n",
       "      <td>[True, True, True, True, False, True, True, Fa...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, B-STREET_A...</td>\n",
       "      <td>[1, 897, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[12, 12, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17903</th>\n",
       "      <td>Yadiel Pate; yadiel_p@mail.com; ROLL NO 307955...</td>\n",
       "      <td>jtwydkjjiw</td>\n",
       "      <td>[Yadiel, Pate, ;, yadiel_p@mail.com, ;, ROLL, ...</td>\n",
       "      <td>[True, False, True, False, True, True, True, F...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, B-EMAIL, O...</td>\n",
       "      <td>[1, 1124, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[12, 12, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21182</th>\n",
       "      <td>Anna Saleh-Parish; 59813 Weaver Neck Suite 623...</td>\n",
       "      <td>taxrdujluv</td>\n",
       "      <td>[Anna, Saleh, -, Parish, ;, 59813, Weaver, Nec...</td>\n",
       "      <td>[True, False, False, False, True, True, True, ...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, I-NAME_STUDEN...</td>\n",
       "      <td>[1, 577, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[12, 12, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101379</th>\n",
       "      <td>Challenge\\n\\nThe challenge is to introduce the...</td>\n",
       "      <td>13539</td>\n",
       "      <td>[Challenge, \\n\\n, The, challenge, is, to, intr...</td>\n",
       "      <td>[False, False, True, True, True, True, True, T...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 6738, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[12, 12, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101400</th>\n",
       "      <td>Example Reflection – Mind Mapping\\n\\nChallenge...</td>\n",
       "      <td>13547</td>\n",
       "      <td>[Example, Reflection, –, Mind, Mapping, \\n\\n, ...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 5736, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[12, 12, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101411</th>\n",
       "      <td>Example Reflection – Mind Mapping\\n\\nChallenge...</td>\n",
       "      <td>13547</td>\n",
       "      <td>[Example, Reflection, –, Mind, Mapping, \\n\\n, ...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 22459, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[12, 12, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101417</th>\n",
       "      <td>Example Reflection – Mind Mapping\\n\\nChallenge...</td>\n",
       "      <td>13547</td>\n",
       "      <td>[Example, Reflection, –, Mind, Mapping, \\n\\n, ...</td>\n",
       "      <td>[True, True, True, True, False, False, True, F...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 17798, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[12, 12, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101447</th>\n",
       "      <td>Storytelling\\n\\nChallenge &amp; Selection:\\n\\nEver...</td>\n",
       "      <td>13551</td>\n",
       "      <td>[Storytelling, \\n\\n, Challenge, &amp;, Selection, ...</td>\n",
       "      <td>[False, False, True, True, False, False, False...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 53779, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[12, 12, 12]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4451 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                full_text    document  \\\n",
       "15384   Charles-Edouard Siegel | c-siegel@outlook.com;...  wcyzegeled   \n",
       "16630   Domenico Childers - 26973 Jennifer Ford\\nJimmy...  kvyrxtluau   \n",
       "17711   Johanny Arnold | PSC 3616, Box 9945\\nAPO AE 52...  cyherfmagt   \n",
       "17903   Yadiel Pate; yadiel_p@mail.com; ROLL NO 307955...  jtwydkjjiw   \n",
       "21182   Anna Saleh-Parish; 59813 Weaver Neck Suite 623...  taxrdujluv   \n",
       "...                                                   ...         ...   \n",
       "101379  Challenge\\n\\nThe challenge is to introduce the...       13539   \n",
       "101400  Example Reflection – Mind Mapping\\n\\nChallenge...       13547   \n",
       "101411  Example Reflection – Mind Mapping\\n\\nChallenge...       13547   \n",
       "101417  Example Reflection – Mind Mapping\\n\\nChallenge...       13547   \n",
       "101447  Storytelling\\n\\nChallenge & Selection:\\n\\nEver...       13551   \n",
       "\n",
       "                                                   tokens  \\\n",
       "15384   [Charles, -, Edouard, Siegel, |, c-siegel@outl...   \n",
       "16630   [Domenico, Childers, -, 26973, Jennifer, Ford,...   \n",
       "17711   [Johanny, Arnold, |, PSC, 3616, ,, Box, 9945, ...   \n",
       "17903   [Yadiel, Pate, ;, yadiel_p@mail.com, ;, ROLL, ...   \n",
       "21182   [Anna, Saleh, -, Parish, ;, 59813, Weaver, Nec...   \n",
       "...                                                   ...   \n",
       "101379  [Challenge, \\n\\n, The, challenge, is, to, intr...   \n",
       "101400  [Example, Reflection, –, Mind, Mapping, \\n\\n, ...   \n",
       "101411  [Example, Reflection, –, Mind, Mapping, \\n\\n, ...   \n",
       "101417  [Example, Reflection, –, Mind, Mapping, \\n\\n, ...   \n",
       "101447  [Storytelling, \\n\\n, Challenge, &, Selection, ...   \n",
       "\n",
       "                                      trailing_whitespace  \\\n",
       "15384   [False, False, True, True, True, False, True, ...   \n",
       "16630   [True, True, True, True, True, False, False, F...   \n",
       "17711   [True, True, True, True, False, True, True, Fa...   \n",
       "17903   [True, False, True, False, True, True, True, F...   \n",
       "21182   [True, False, False, False, True, True, True, ...   \n",
       "...                                                   ...   \n",
       "101379  [False, False, True, True, True, True, True, T...   \n",
       "101400  [True, True, True, True, False, False, True, F...   \n",
       "101411  [True, True, True, True, False, False, True, F...   \n",
       "101417  [True, True, True, True, False, False, True, F...   \n",
       "101447  [False, False, True, True, False, False, False...   \n",
       "\n",
       "                                                   labels   berttokenids  \\\n",
       "15384   [B-NAME_STUDENT, I-NAME_STUDENT, I-NAME_STUDEN...   [1, 5736, 2]   \n",
       "16630   [B-NAME_STUDENT, I-NAME_STUDENT, O, B-STREET_A...  [1, 10766, 2]   \n",
       "17711   [B-NAME_STUDENT, I-NAME_STUDENT, O, B-STREET_A...    [1, 897, 2]   \n",
       "17903   [B-NAME_STUDENT, I-NAME_STUDENT, O, B-EMAIL, O...   [1, 1124, 2]   \n",
       "21182   [B-NAME_STUDENT, I-NAME_STUDENT, I-NAME_STUDEN...    [1, 577, 2]   \n",
       "...                                                   ...            ...   \n",
       "101379  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   [1, 6738, 2]   \n",
       "101400  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   [1, 5736, 2]   \n",
       "101411  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [1, 22459, 2]   \n",
       "101417  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [1, 17798, 2]   \n",
       "101447  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  [1, 53779, 2]   \n",
       "\n",
       "       berttokenmask berttokentoken_type_ids    bertlabels  \n",
       "15384      [1, 1, 1]               [0, 0, 0]  [12, 12, 12]  \n",
       "16630      [1, 1, 1]               [0, 0, 0]  [12, 12, 12]  \n",
       "17711      [1, 1, 1]               [0, 0, 0]  [12, 12, 12]  \n",
       "17903      [1, 1, 1]               [0, 0, 0]  [12, 12, 12]  \n",
       "21182      [1, 1, 1]               [0, 0, 0]  [12, 12, 12]  \n",
       "...              ...                     ...           ...  \n",
       "101379     [1, 1, 1]               [0, 0, 0]  [12, 12, 12]  \n",
       "101400     [1, 1, 1]               [0, 0, 0]  [12, 12, 12]  \n",
       "101411     [1, 1, 1]               [0, 0, 0]  [12, 12, 12]  \n",
       "101417     [1, 1, 1]               [0, 0, 0]  [12, 12, 12]  \n",
       "101447     [1, 1, 1]               [0, 0, 0]  [12, 12, 12]  \n",
       "\n",
       "[4451 rows x 9 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rebuid dataset \n",
    "tmp_pd=expanddataset(preprocesssed_ds)\n",
    "print(tmp_pd['berttokenids'].str.len().agg(['mean','max','std','min']))\n",
    "tmp_pd.loc[tmp_pd['berttokenids'].str.len()==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['full_text', 'document', 'tokens', 'trailing_whitespace', 'labels', 'berttokenids', 'berttokenmask', 'berttokentoken_type_ids', 'bertlabels'],\n",
      "    num_rows: 101484\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "full_ds=Dataset.from_pandas(tmp_pd)\n",
    "print(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_len=int(0.9*len(full_ds)) # 长度分割，兼容老版本torch\n",
    "train_dataset, val_dataset = random_split(full_ds, [train_len,len(full_ds)-train_len],generator=torch.Generator().manual_seed(Config.seed)) #TODO:改变比例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset len:91335\n",
      "vail dataset len:10149\n"
     ]
    }
   ],
   "source": [
    "print(f\"train dataset len:{len(train_dataset)}\")\n",
    "print(f\"vail dataset len:{len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = Collate(tokenizer=tokenizer)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=Config.batch_size,pin_memory=True,collate_fn=data_collator,shuffle=True)\n",
    "val_dataloader=DataLoader(val_dataset,batch_size=Config.batch_size,pin_memory=True,collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from model1 import NLPModel\n",
    "model=NLPModel(id2label,label2id,Config.model_name).to(device)\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from seqeval.metrics import recall_score, precision_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "optimizer= AdamW(model.parameters(),lr=Config.lr,weight_decay=Config.weight_decay)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "valstep = len(train_dataloader) //Config.evaltimes\n",
    "\n",
    "num_train_steps = int(len(train_dataloader) * Config.epochs)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=Config.num_warmup_steps, num_training_steps=num_train_steps, \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions,traget, all_labels):\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, traget)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, traget)\n",
    "    ]\n",
    "    \n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "    \n",
    "    results = {\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1_score\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def validation(val_dataloader,model):\n",
    "    model.eval()\n",
    "\n",
    "    allprecitions=[]\n",
    "    alltargets=[]\n",
    "    for step,dataset in enumerate(tqdm(val_dataloader)):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ids = dataset[\"ids\"].to(device,non_blocking=True)\n",
    "            mask = dataset[\"mask\"].to(device,non_blocking=True)\n",
    "            targets = dataset[\"targets\"].to(device,non_blocking=True)\n",
    "            tokentype = dataset[\"type_ids\"].to(device,non_blocking=True)\n",
    "            logit,loss = model(ids,mask, tokentype,targets)\n",
    "\n",
    "            targets=targets.view(-1)\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "\n",
    "            allprecitions.append(np.argmax(logit, axis=1))\n",
    "            alltargets.append(targets)\n",
    "        del ids,mask,targets,tokentype,loss,logit\n",
    "\n",
    "    score=compute_metrics(allprecitions,alltargets,all_labels)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/39539 [00:24<67:33:22,  6.15s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     targets \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device,non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m     logit,loss \u001b[38;5;241m=\u001b[39m model(ids,mask, tokentype,targets)\n\u001b[0;32m---> 28\u001b[0m     loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39maccumulation_steps\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Accumulates scaled gradients.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[31], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     targets \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device,non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m     logit,loss \u001b[38;5;241m=\u001b[39m model(ids,mask, tokentype,targets)\n\u001b[0;32m---> 28\u001b[0m     loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39maccumulation_steps\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Accumulates scaled gradients.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "start_time=time.time()\n",
    "bestscore=0\n",
    "epoch=0\n",
    "\n",
    "if Config.resume_train_epoch:\n",
    "    checkpoint = torch.load(Config.workdir+f\"//checkpoint//checkpoint{Config.resume_train_epoch}.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    bestscore=checkpoint['bestscore']\n",
    "    model.train()\n",
    "    print(f\"start epoch:{epoch}\")\n",
    "\n",
    "while epoch <Config.epochs:\n",
    "    losses=[] \n",
    "    for step,dataset in enumerate(tqdm(train_dataloader)):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            ids = dataset[\"ids\"].to(device,non_blocking=True)\n",
    "            mask = dataset[\"mask\"].to(device,non_blocking=True)\n",
    "            tokentype = dataset[\"type_ids\"].to(device,non_blocking=True)\n",
    "            targets = dataset[\"targets\"].to(device,non_blocking=True)\n",
    "\n",
    "            logit,loss = model(ids,mask, tokentype,targets)\n",
    "            loss/= Config.accumulation_steps\n",
    "\n",
    "        # Accumulates scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "        # losses.append(loss.item())\n",
    "        \n",
    "        if (step+1) % Config.accumulation_steps ==0:\n",
    "            scaler.step(optimizer)\n",
    "            scale = scaler.get_scale()\n",
    "            # Updates the scale for next iteration.\n",
    "            scaler.update()\n",
    "            skip_lr_sched = (scale != scaler.get_scale())\n",
    "            if not skip_lr_sched:\n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # validation\n",
    "        if epoch>0 and (step+1) % valstep == 0:\n",
    "            scores=validation(val_dataloader,model)\n",
    "            print(\"Validation:Epoch{} Step [{}/{}] Loss: {:.3f} Time: {:.1f} Metric: {}\".format(epoch,step, len(train_dataloader)-1, loss.item(), time.time()-start_time,scores))\n",
    "            if scores['f1']>bestscore:\n",
    "                print(f\"Best score is {bestscore} → {scores['f1']}. Saving model\")\n",
    "                torch.save(model.state_dict(), os.path.join(Config.modelsavepath,\"model_seed{}_score{:.3f}.pth\".format(Config.seed,bestscore)))\n",
    "                bestscore=scores['f1']\n",
    "            else:\n",
    "                print(\"no improvement, bestf1 is {:.3f} score is {}\".format(bestscore,scores))\n",
    "                # report loss\n",
    "        if (step+1) % Config.logging_steps ==0:\n",
    "            # print(\"Logging: Step [{}/{}] Loss: {:.3f} Time: {:.1f}\".format(step, len(train_dataloader)-1, loss.item(), time.time()-start_time))\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        del ids,mask,targets,tokentype,loss,logit\n",
    "\n",
    "    epoch+=1\n",
    "    # checkpoint\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'bestscore':bestscore,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict':scheduler.state_dict(),\n",
    "            'scaler_state_dict':scaler.state_dict()\n",
    "            }, Config.modelsavepath+\"/checkpoint{}.pth\".format(epoch))\n",
    "\n",
    "tokenizer.save_pretrained(Config.modelsavepath)\n",
    "os.system(\"/usr/bin/shutdown\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
