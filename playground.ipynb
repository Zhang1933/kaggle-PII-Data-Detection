{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "# output = result.stdout\n",
    "# for line in output.splitlines():\n",
    "#     if '=' in line:\n",
    "#         var, value = line.split('=', 1)\n",
    "#         os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir -p modelsave checkpoint/\n",
    "# %pip install --force-reinstall /home/z1933/workplace/machinelearning/PII-Data-Detection/unidecode/Unidecode-1.3.8-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfg import *\n",
    "from util import *\n",
    "Config.modelsavepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### main\n",
    "data = json.load(open(Config.data_path+\"/train.json\"))\n",
    "\n",
    "# downsampling of negative examples\n",
    "# p=[] # positive samples (contain relevant labels)\n",
    "# n=[] # negative samples (presumably contain entities that are possibly wrongly classified as entity)\n",
    "# for d in data:\n",
    "#     if any(np.array(d[\"labels\"]) != \"O\"): p.append(d)\n",
    "#     else: n.append(d)\n",
    "# print(\"original datapoints: \", len(data))\n",
    "\n",
    "external = json.load(open(Config.data_path+\"/mixtral-8x7b-v1.json\"))\n",
    "print(\"external datapoints: \", len(external))\n",
    "\n",
    "# moredata = json.load(open(Config.data_path+\"/moredata_dataset_fixed.json\"))\n",
    "# print(\"moredata datapoints: \", len(moredata))\n",
    "\n",
    "data = data+external\n",
    "# print(\"combined: \", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd=pd.DataFrame(data)\n",
    "len(data_pd[data_pd['tokens'].apply(len)>2024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
    "tokenized_input =tokenizer(data_pd.iloc[data_pd.full_text.str.len().idxmax()]['full_text'],return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokenized_input[\"input_ids\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "#将所有数据的labels连接在一起,然后查重,转成list的格式,然后从小到大排序\n",
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "print(f\"all_labels:{all_labels}\")\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "#这个是{id:label},上面是{label:id}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "# Open a file for writing\n",
    "with open(Config.modelsavepath+\"//idlabel.json\", \"w\") as f:\n",
    "    # Write the map to the file in JSON format\n",
    "    json.dump({'label2id':label2id,'id2label':id2label}, f)\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "train_df = data\n",
    "print(f\"len(train_df):{len(train_df)}, train_df[0].keys(): {list(train_df[0].keys())}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "print(f\"train_df length:\", len(train_df))\n",
    "\n",
    "labels = set()\n",
    "label_counts = {}\n",
    "for i in range(len(train_df)):\n",
    "    labels.update(train_df[i]['labels'])\n",
    "    for label in train_df[i]['labels']:\n",
    "        if label in label_counts:\n",
    "            label_counts[label] += 1\n",
    "        else:\n",
    "            label_counts[label] = 1\n",
    "            \n",
    "print(f\"labels: {labels}\")\n",
    "print('-'*25)\n",
    "print(f\"label_counts: {label_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # EDA\n",
    "# display bar\n",
    "# import seaborn as sns\n",
    "# import copy\n",
    "# display_label=copy.copy(label_counts)\n",
    "# del  display_label['O']\n",
    "# y =  [i for i in display_label.values()]\n",
    "\n",
    "# g=sns.barplot(x=list(display_label.keys()),y=y)\n",
    "# g.set_xticklabels(g.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=dict()\n",
    "for i in train_df:\n",
    "    for j in range(0,len(i['tokens'])):\n",
    "        if not i['tokens'][j].isascii():\n",
    "            if i['tokens'][j] in tmp:\n",
    "                tmp[i['tokens'][j]]+=1\n",
    "            else:\n",
    "                tmp[i['tokens'][j]]=0\n",
    "dict(sorted(tmp.items(), key=lambda x: x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.DataFrame(train_df)\n",
    "df_train_tokens=[]\n",
    "for row in df_train.itertuples(index=False):\n",
    "    for i in range(len(row.tokens)):\n",
    "        df_train_tokens.append((row.document,row.tokens[i],row.labels[i],row.trailing_whitespace[i]))\n",
    "df_train_tokens=pd.DataFrame(df_train_tokens,columns=['document','tokens','labels','trailing_whitespace'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_tokens.loc[(df_train_tokens['tokens']=='Geoff')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEA\n",
    "# print all unicode character\n",
    "# def has_unicode_codepoints(text):\n",
    "#     return any(ord(char) > 127 for char in text)\n",
    "# tmp_set=set()\n",
    "# pattern =re.compile(r\"[^\\u0000-\\u00ff]\")\n",
    "# for i in data:\n",
    "#     for j in i['tokens']:\n",
    "#         if has_unicode_codepoints(j):\n",
    "#             tmp_set.add(j)\n",
    "# for i in tmp_set:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp ./nltk_data/* ~/nltk_data/ -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [str(x[\"document\"]) for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "    \"labels\": [x[\"labels\"] for x in data]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测时，空缺值填'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# tmp_list=[]\n",
    "# for doc in data:\n",
    "#     full_text=doc['full_text']\n",
    "#     split_lines=full_text.split('\\n\\n')\n",
    "#     for i in split_lines:\n",
    "#         tmp_list.append(len(tokenizer(i)['input_ids']))\n",
    "# print(len(tmp_list))\n",
    "# print(max(tmp_list))\n",
    "# sns.displot(tmp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据'\\n\\n'分token\n",
    "preprocesssed_ds=ds.map(train_preprocesss, fn_kwargs={'tokenizer':tokenizer,'label2id':label2id},num_proc=Config.num_proc,desc=\"prepocessing data\")\n",
    "preprocesssed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_str=\".\"\n",
    "tmp_res=tokenizer(tmp_str)\n",
    "tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(tmp_res['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuid dataset \n",
    "tmp_pd=expanddataset(preprocesssed_ds)\n",
    "print(tmp_pd['berttokenids'].str.len().agg(['mean','max','std','min']))\n",
    "print(\"len after expand\",len(tmp_pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_pd.loc[tmp_pd['berttokenids'].str.len()==20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds=Dataset.from_pandas(tmp_pd)\n",
    "print(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_len=int(0.75*len(full_ds)) # 长度分割，兼容老版本torch\n",
    "train_dataset, val_dataset = random_split(full_ds, [train_len,len(full_ds)-train_len],generator=torch.Generator().manual_seed(Config.seed)) #TODO:改变比例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train dataset len:{len(train_dataset)}\")\n",
    "print(f\"vail dataset len:{len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = Collate(tokenizer=tokenizer)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=Config.batch_size,pin_memory=True,collate_fn=data_collator,shuffle=True)\n",
    "val_dataloader=DataLoader(val_dataset,batch_size=Config.batch_size,pin_memory=True,collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model1 import NLPModel\n",
    "model=NLPModel(id2label,label2id,Config.model_name).to(device)\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from seqeval.metrics import recall_score, precision_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "optimizer= AdamW(model.parameters(),lr=Config.lr,weight_decay=Config.weight_decay)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "valstep = len(train_dataloader) //Config.evaltimes\n",
    "\n",
    "num_train_steps = int(len(train_dataloader) * Config.epochs)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=Config.num_warmup_steps, num_training_steps=num_train_steps, \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions,traget, all_labels):\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, traget)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, traget)\n",
    "    ]\n",
    "    \n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "    \n",
    "    results = {\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1_score\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def validation(val_dataloader,model):\n",
    "    model.eval()\n",
    "\n",
    "    allprecitions=[]\n",
    "    alltargets=[]\n",
    "    for step,dataset in enumerate(tqdm(val_dataloader)):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ids = dataset[\"ids\"].to(device,non_blocking=True)\n",
    "            mask = dataset[\"mask\"].to(device,non_blocking=True)\n",
    "            targets = dataset[\"targets\"].to(device,non_blocking=True)\n",
    "            tokentype = dataset[\"type_ids\"].to(device,non_blocking=True)\n",
    "            logit,loss = model(ids,mask, tokentype,targets)\n",
    "\n",
    "            targets=targets.view(-1)\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "\n",
    "            allprecitions.append(np.argmax(logit, axis=1))\n",
    "            alltargets.append(targets)\n",
    "        del ids,mask,targets,tokentype,loss,logit\n",
    "\n",
    "    score=compute_metrics(allprecitions,alltargets,all_labels)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(Config.modelsavepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "start_time=time.time()\n",
    "bestscore=0\n",
    "epoch=0\n",
    "\n",
    "if Config.resume_train_epoch:\n",
    "    checkpoint = torch.load(Config.workdir+f\"//checkpoint//checkpoint{Config.resume_train_epoch}.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    bestscore=checkpoint['bestscore']\n",
    "    model.train()\n",
    "    print(f\"start epoch:{epoch}\")\n",
    "\n",
    "while epoch <Config.epochs:\n",
    "    losses=[] \n",
    "    for step,dataset in enumerate(tqdm(train_dataloader)):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            ids = dataset[\"ids\"].to(device,non_blocking=True)\n",
    "            mask = dataset[\"mask\"].to(device,non_blocking=True)\n",
    "            tokentype = dataset[\"type_ids\"].to(device,non_blocking=True)\n",
    "            targets = dataset[\"targets\"].to(device,non_blocking=True)\n",
    "\n",
    "            logit,loss = model(ids,mask, tokentype,targets)\n",
    "            loss/= Config.accumulation_steps\n",
    "\n",
    "        # Accumulates scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "        # losses.append(loss.item())\n",
    "        \n",
    "        if (step+1) % Config.accumulation_steps ==0:\n",
    "            scaler.step(optimizer)\n",
    "            scale = scaler.get_scale()\n",
    "            # Updates the scale for next iteration.\n",
    "            scaler.update()\n",
    "            skip_lr_sched = (scale != scaler.get_scale())\n",
    "            if not skip_lr_sched:\n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # validation\n",
    "        if epoch>0 and (step+1) % valstep == 0:\n",
    "            scores=validation(val_dataloader,model)\n",
    "            print(\"Validation:Epoch{} Step [{}/{}] Loss: {} Time: {:.1f} Metric: {}\".format(epoch,step, len(train_dataloader)-1, loss.item(), time.time()-start_time,scores))\n",
    "            if scores['f1']>bestscore:\n",
    "                print(f\"Best score is {bestscore} → {scores['f1']}. Saving model\")\n",
    "                bestscore=scores['f1']\n",
    "                torch.save(model.state_dict(), os.path.join(Config.modelsavepath,\"model_seed{}_score{:.3f}.pth\".format(Config.seed,bestscore)))\n",
    "            else:\n",
    "                print(\"no improvement, bestf1 is {:.3f} score is {}\".format(bestscore,scores))\n",
    "                # report loss\n",
    "        if (step+1) % Config.logging_steps ==0:\n",
    "            # print(\"Logging: Step [{}/{}] Loss: {:.3f} Time: {:.1f}\".format(step, len(train_dataloader)-1, loss.item(), time.time()-start_time))\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        del ids,mask,targets,tokentype,loss,logit\n",
    "\n",
    "    epoch+=1\n",
    "    # checkpoint\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'bestscore':bestscore,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict':scheduler.state_dict(),\n",
    "            'scaler_state_dict':scaler.state_dict()\n",
    "            }, Config.modelsavepath+\"/checkpoint{}.pth\".format(epoch))\n",
    "\n",
    "tokenizer.save_pretrained(Config.modelsavepath)\n",
    "os.system(\"/usr/bin/shutdown\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
