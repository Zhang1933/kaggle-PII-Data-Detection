{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://www.kaggle.com/code/conjuring92/pii-metric-fine-grained-eval#Version-3](PII Metric - Fine Grained Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class PRFScore:\n",
    "    \"\"\"A precision / recall / F score.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        tp: int = 0,\n",
    "        fp: int = 0,\n",
    "        fn: int = 0,\n",
    "    ) -> None:\n",
    "        self.tp = tp\n",
    "        self.fp = fp\n",
    "        self.fn = fn\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.tp + self.fp + self.fn\n",
    "\n",
    "    def __iadd__(self, other):  # in-place add\n",
    "        self.tp += other.tp\n",
    "        self.fp += other.fp\n",
    "        self.fn += other.fn\n",
    "        return self\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return PRFScore(\n",
    "            tp=self.tp + other.tp, fp=self.fp + other.fp, fn=self.fn + other.fn\n",
    "        )\n",
    "\n",
    "    def score_set(self, cand: set, gold: set) -> None:\n",
    "        self.tp += len(cand.intersection(gold))\n",
    "        self.fp += len(cand - gold)\n",
    "        self.fn += len(gold - cand)\n",
    "\n",
    "    @property\n",
    "    def precision(self) -> float:\n",
    "        return self.tp / (self.tp + self.fp + 1e-100)\n",
    "\n",
    "    @property\n",
    "    def recall(self) -> float:\n",
    "        return self.tp / (self.tp + self.fn + 1e-100)\n",
    "\n",
    "    @property\n",
    "    def f1(self) -> float:\n",
    "        p = self.precision\n",
    "        r = self.recall\n",
    "        return 2 * ((p * r) / (p + r + 1e-100))\n",
    "\n",
    "    @property\n",
    "    def f5(self) -> float:\n",
    "        beta = 5\n",
    "        p = self.precision\n",
    "        r = self.recall\n",
    "\n",
    "        fbeta = (1+(beta**2))*p*r / ((beta**2)*p + r + 1e-100)\n",
    "        return fbeta\n",
    "\n",
    "    def to_dict(self) -> Dict[str, float]:\n",
    "        return {\"p\": self.precision, \"r\": self.recall, \"f5\": self.f5}\n",
    "\n",
    "\n",
    "def compute_metrics(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    Compute the LB metric (lb) and other auxiliary metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    references = {(row.document, row.token, row.label) for row in gt_df.itertuples()}\n",
    "    predictions = {(row.document, row.token, row.label) for row in pred_df.itertuples()}\n",
    "\n",
    "    score_per_type = defaultdict(PRFScore)\n",
    "    references = set(references)\n",
    "\n",
    "    for ex in predictions:\n",
    "        pred_type = ex[-1] # (document, token, label)\n",
    "        if pred_type != 'O':\n",
    "            pred_type = pred_type[2:] # avoid B- and I- prefix\n",
    "            \n",
    "        if pred_type not in score_per_type:\n",
    "            score_per_type[pred_type] = PRFScore()\n",
    "\n",
    "        if ex in references:\n",
    "            score_per_type[pred_type].tp += 1\n",
    "            references.remove(ex)\n",
    "        else:\n",
    "            score_per_type[pred_type].fp += 1\n",
    "\n",
    "    for doc, tok, ref_type in references:\n",
    "        if ref_type != 'O':\n",
    "            ref_type = ref_type[2:] # avoid B- and I- prefix\n",
    "        \n",
    "        if ref_type not in score_per_type:\n",
    "            score_per_type[ref_type] = PRFScore()\n",
    "        score_per_type[ref_type].fn += 1\n",
    "\n",
    "    totals = PRFScore()\n",
    "    \n",
    "    for prf in score_per_type.values():\n",
    "        totals += prf\n",
    "\n",
    "    return {\n",
    "        \"ents_p\": totals.precision,\n",
    "        \"ents_r\": totals.recall,\n",
    "        \"ents_f5\": totals.f5,\n",
    "        \"ents_per_type\": {k: v.to_dict() for k, v in score_per_type.items() if k!= 'O'},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data and create reference dataframe ---\n",
    "\n",
    "reference_data = json.load(open(\"./dataset/train.json\", \"r\"))\n",
    "df = pd.DataFrame(reference_data)[['document', 'tokens', 'labels']].copy()\n",
    "df = df.explode(['tokens', 'labels']).reset_index(drop=True).rename(columns={'tokens': 'token', 'labels': 'label'})\n",
    "df['token'] = df.groupby('document').cumcount()\n",
    "\n",
    "label_list = df['label'].unique().tolist()\n",
    "\n",
    "reference_df = df[df['label'] != 'O'].copy()\n",
    "reference_df = reference_df.reset_index().rename(columns={'index': 'row_id'})\n",
    "reference_df = reference_df[['row_id', 'document', 'token', 'label']].copy()\n",
    "\n",
    "reference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df=pd.read_csv(\"./submission.csv\")\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = compute_metrics(pred_df, reference_df)\n",
    "m = eval_dict['ents_f5']\n",
    "print(f\"LB = {round(m, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine grained results\n",
    "print(json.dumps(eval_dict['ents_per_type'], indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
